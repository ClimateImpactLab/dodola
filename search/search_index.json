{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#dodola","title":"dodola","text":"<p>Containerized application for running individual tasks in a larger, orchestrated CMIP6 bias-adjustment and downscaling workflow.</p> <p>This is under heavy development.</p>"},{"location":"#features","title":"Features","text":"<p>Commands can be run through the command line with <code>dodola &lt;command&gt;</code>.</p> <pre><code>Commands:\n    adjust-maximum-precipitation  Adjust maximum precipitation in a dataset\n    apply-dtr-floor               Apply a floor to diurnal temperature...\n    apply-non-polar-dtr-ceiling   Apply a ceiling to diurnal temperature...\n    apply-qdm                     Adjust simulation year with quantile...\n    apply-qplad                   Adjust (downscale) simulation year with...\n    cleancmip6                    Clean up and standardize GCM\n    correct-wetday-frequency      Correct wet day frequency in a dataset\n    get-attrs                     Get attrs from data\n    prime-qdm-output-zarrstore    Prime a Zarr Store for regionally-written...\n    prime-qplad-output-zarrstore  Prime a Zarr Store for regionally-written...\n    rechunk                       Rechunk Zarr store in memory.\n    regrid                        Spatially regrid a Zarr Store in memory\n    removeleapdays                Remove leap days and update calendar\n    train-qdm                     Train quantile delta mapping (QDM)\n    train-qplad                   Train Quantile-Preserving, Localized...\n    validate-dataset              Validate a CMIP6, bias corrected or...\n</code></pre> <p>See <code>dodola --help</code> or <code>dodola &lt;command&gt; --help</code> for more information.</p>"},{"location":"#example","title":"Example","text":"<p>From the command line, run one of the downscaling workflow's validation steps with: </p> <pre><code>dodola validate-dataset \"gs://your/climate/data.zarr\" \\\n  --variable \"tasmax\" \\\n  --data-type \"downscaled\" \\\n  -t \"historical\"\n</code></pre> <p>The service used by this command can be called directly from a Python session or script</p> <pre><code>import dodola.services\n\ndodola.services.validate(\n    \"gs://your/climate/data.zarr\", \n    \"tasmax\",\n    data_type=\"downscaled\",\n    time_period=\"historical\",\n)\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<p><code>dodola</code> is generally run from within a container. <code>dodola</code> container images are currently hosted at ghcr.io/climateimpactlab/dodola.</p> <p>Alternatively, you can install a bleeding-edge version of the application and access the command-line interface or Python API with <code>pip</code>:</p> <pre><code>pip install git+https://github.com/ClimateImpactLab/dodola\n</code></pre> <p>Because there are many compiled dependencies we recommend installing <code>dodola</code> and its dependencies within a <code>conda</code> virtual environment. Dependencies used in the container to create its <code>conda</code> environment are in <code>./environment.yaml</code>.</p>"},{"location":"#support","title":"Support","text":"<p>Additional technical documentation is available online at https://climateimpactlab.github.io/dodola/.</p> <p>Source code is available online at https://github.com/ClimateImpactLab/dodola. This software is Open Source and available under the Apache License, Version 2.0.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Add documentation, hosted at https://climateimpactlab.github.io/dodola/. (PR #235, PR #205, @brews)</li> <li>Test coverage for container tests in CI. (PR #188, PR #191, @brews)</li> <li>Add <code>org.opencontainers.image</code> labels with metadata to container. (PR #195, PR #233, @brews)</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Bleeding-edge development container now uses \"edge\" tag. \"dev\" tag no longer updated. (PR #233, @brews)</li> <li>Minor clarifications to docs and comments. (PR #232, @brews)</li> <li>Migrate parent container from <code>miniconda3</code> to <code>micromamba</code>. Note this is a significant change to the container environment which may break scripts run in the container. In addition, the containers entry point has changed, breaking backwards compatibility for Argo Workflows running with Emissary executors. (PR #195, @brews)</li> <li>Migrate Python package metadata and dependencies from <code>setup.cfg</code> to <code>pyproject.toml</code>. Note the runtime environment used in the container is still <code>environment.yaml</code>. (PR #195, @brews)</li> <li>Minor README updates, improvements. (PR #195, @brews)</li> <li>Updates to Python package metadata, specific dependencies, versioning, classifiers, project URLs. (PR #206, @brews)</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Fix broken release links in CHANGELOG. (@brews)</li> </ul>"},{"location":"changelog/#0190-2022-03-25","title":"0.19.0 - 2022-03-25","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Add badge for current release DOI to README. (@brews)</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Significant update to container/package environment pins. (PR #183, @brews) </li> </ul>"},{"location":"changelog/#0180-2022-03-03","title":"0.18.0 - 2022-03-03","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Add basic CI/CD test and build status badges to README. (PR #182, @brews)</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Fix dodola validate-dataset OOM on small workers without dask-distributed. (PR #181, @brews)</li> </ul>"},{"location":"changelog/#0170-2022-02-17","title":"0.17.0 - 2022-02-17","text":""},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Increase max allowed tasmin, tasmax in services.validate to 377 K for UKESM1-0-LL. (PR #180, @brews)</li> </ul>"},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Move in-memory data loading where it is needed for 360-days calendar conversion in clean-cmip6 (PR #179, @emileten)</li> </ul>"},{"location":"changelog/#0162-2022-02-15","title":"0.16.2 - 2022-02-15","text":""},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Fix incorrect references when standardizing gcm (PR #178, @emileten)</li> </ul>"},{"location":"changelog/#0161-2022-01-27","title":"0.16.1 - 2022-01-27","text":""},{"location":"changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Fix the wetday frequency correction so that different replacement values are used, rather than a single one (PR #174, PR #176, @emileten, @delgadom).</li> </ul>"},{"location":"changelog/#0160-2022-01-19","title":"0.16.0 - 2022-01-19","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Improve README.md. (PR #169, @brews)</li> </ul>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>Remove duplicated service-level logging info lines of code introduced by PR #148 (PR #168, @emileten)</li> <li>Decrease validation temperature range min to 130 (PR #170, @emileten)</li> </ul>"},{"location":"changelog/#0151-2021-12-29","title":"0.15.1 - 2021-12-29","text":""},{"location":"changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Fix boolean condition error in DTR ceiling application (PR #165, PR #166, @emileten)</li> </ul>"},{"location":"changelog/#0150-2021-12-27","title":"0.15.0 - 2021-12-27","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Add maximum precipitation adjustment service that applies a \"ceiling\" or \"cap\" to precipitation values above a user-defined threshold. (PR #164, @dgergel)</li> </ul>"},{"location":"changelog/#changed_4","title":"Changed","text":"<ul> <li>Increase max precipitation allowed by validation to 3000 mm. (PR #164, @dgergel)</li> <li>Update wet day frequency correction to incorporate method additions from Hempel et al 2013. (PRs #162 and #159, @dgergel)</li> <li>Floor and ceiling for DTR. (PR #163 @emileten)</li> </ul>"},{"location":"changelog/#0140-2021-12-21","title":"0.14.0 - 2021-12-21","text":""},{"location":"changelog/#changed_5","title":"Changed","text":"<ul> <li>Update wet day frequency correction to include small negative values in correction and to limit the correction range to the threshold * 10 ^ -2. (PR #158, @dgergel)</li> <li>Update package setup, README, HISTORY/CHANGELOG to new system. (PR #154, @brews)</li> </ul>"},{"location":"changelog/#0130-2021-12-17","title":"0.13.0 - 2021-12-17","text":""},{"location":"changelog/#changed_6","title":"Changed","text":"<ul> <li>Update diurnal temperature range (DTR) validation to differentiate polar and non-polar regions. (PR #153, @dgergel)</li> <li>Update diurnal temperature range (DTR) validation to differentiate min DTR accepted value for CMIP6 vs bias corrected and downscaled data inputs (PR #155, @dgergel)</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>Remove cruft code. Remove <code>dodola</code> commands <code>biascorrect</code>, <code>downscale</code>, <code>buildweights</code> along with corresponding functions in <code>dodola.services</code> and <code>dodola.core</code>.  (PR #152, @brews)</li> </ul>"},{"location":"changelog/#fixed_6","title":"Fixed","text":"<ul> <li>Fix rechunk error when converting 360 days calendars. (#149, PR #151, @brews)</li> </ul>"},{"location":"changelog/#0120-2021-12-09","title":"0.12.0 - 2021-12-09","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li>Add 360 days calendar support (PR #144, @emileten)</li> <li>Add an option to temporarily replace the target variable units in dodola services and use in CLI dodola for precip (PR #143, @emileten)</li> <li>Add diurnal temperature range (DTR) correction for small DTR values below 1 (converts them to 1) (PR #145, @dgergel)</li> </ul>"},{"location":"changelog/#0111-2021-12-03","title":"0.11.1 - 2021-12-03","text":""},{"location":"changelog/#changed_7","title":"Changed","text":"<ul> <li>Decrease allowed timesteps for bias corrected/downscaled files in validation to allow models that only go through 2099 (PR #146, @dgergel)</li> </ul>"},{"location":"changelog/#0110-2021-11-30","title":"0.11.0 - 2021-11-30","text":""},{"location":"changelog/#added_6","title":"Added","text":"<ul> <li>Add post wet day correction option in CLI dodola (PR #141 @emileten)</li> </ul>"},{"location":"changelog/#changed_8","title":"Changed","text":"<ul> <li>Increase validation temperature range max to 360 (PR #142, @dgergel)</li> <li>Distinguish missing from excess timesteps in timesteps validation (PR #140, @emileten)</li> </ul>"},{"location":"changelog/#0100-2021-11-22","title":"0.10.0 - 2021-11-22","text":""},{"location":"changelog/#added_7","title":"Added","text":"<ul> <li>Add additional tests for <code>dodola.core.*_analogdownscaling</code> functions. (PR #136, @dgergel, @brews)</li> </ul>"},{"location":"changelog/#changed_9","title":"Changed","text":"<ul> <li>Update dtr range check max to allow up to 70 C. (PR #138, @brews, @dgergel)</li> </ul>"},{"location":"changelog/#090-2021-11-15","title":"0.9.0 - 2021-11-15","text":""},{"location":"changelog/#added_8","title":"Added","text":"<ul> <li>Add <code>--root-attrs-json-file</code> to <code>prime-qplad-output-zarrstore</code>, <code>apply-qplad</code>, <code>prime-qdm-output-zarrstore</code>, <code>apply-qdm</code>. (PR #134, @brews)</li> <li>Add <code>dodola get-attrs</code> command. (PR #133, @brews)</li> </ul>"},{"location":"changelog/#changed_10","title":"Changed","text":"<ul> <li>Upgrade Docker base image to <code>continuumio/miniconda3:4.10.3</code>. (PR #132, @brews)</li> </ul>"},{"location":"changelog/#fixed_7","title":"Fixed","text":"<ul> <li>Fix attrs missing from services.apply_qplad output Datasets. (#135, @brews)</li> </ul>"},{"location":"changelog/#080-2021-11-10","title":"0.8.0 - 2021-11-10","text":""},{"location":"changelog/#added_9","title":"Added","text":"<ul> <li>Add AIQPD output Zarr priming (<code>prime-aipqd-output-zarrstore</code>), input slicing, region writing, attrs merging, and multi-year processing. This breaks backwards compatibility for <code>apply-aiqpd</code> and its services and core functions. See the pull request for additional details. (PR #130, @brews)</li> <li>Similarly, add QDM output Zarr priming (<code>prime-qdm-output-zarrstore</code>), region writing, attrs merging, and multi-year processing. This breaks backwards compatibility for <code>apply-qdm</code> and its services and core functions. See the pull request for additional details. (PR #129, @brews)</li> <li>Add pre-training slicing options to <code>train-qdm</code> and <code>train-aiqpd</code>. (PR #123, PR #128, @brews)</li> </ul>"},{"location":"changelog/#changed_11","title":"Changed","text":"<ul> <li>AIQPD has been renamed \"Quantile-Preserving, Localized Analogs Downscaling\" (QPLAD). AIQPD-named commands have been switch to QPLAD. This is backward compatibility breaking. (PR #131, @brews)</li> <li>Make logging slightly more chatty by default. (PR #129, @brews)</li> </ul>"},{"location":"changelog/#fixed_8","title":"Fixed","text":"<ul> <li>Quick fix validation reading entire zarr store for check. (PR #124, @brews)</li> </ul>"},{"location":"changelog/#070-2021-11-02","title":"0.7.0 - 2021-11-02","text":""},{"location":"changelog/#added_10","title":"Added","text":"<ul> <li>Add global validation, includes new service <code>validate</code> for validating cleaned CMIP6, bias corrected and downscaled data for historical and future time periods. (PR #118, @dgergel)</li> </ul>"},{"location":"changelog/#changed_12","title":"Changed","text":"<ul> <li>Update xclim version to 0.30.1, this updates the Train/Adjust API for QDM and AIQPD and requires units attributes for all QDM and AIQPD inputs. (PR #119, @dgergel)</li> <li>Regrid copies input Dataset <code>attrs</code> metadata to output (#116). (PR #121, @brews)</li> </ul>"},{"location":"changelog/#security","title":"Security","text":"<ul> <li>Upgrade <code>dask</code> to 2021.10.0 to cover https://nvd.nist.gov/vuln/detail/CVE-2021-42343. (PR #122, @brews)</li> </ul>"},{"location":"changelog/#060-2021-09-08","title":"0.6.0 - 2021-09-08","text":""},{"location":"changelog/#added_11","title":"Added","text":"<ul> <li>Add AIQPD downscaling method to options. Also updates <code>xclim</code> dependency to use the CIL-fork and \"@add_analog_downscaling\" branch, with 0.28.1 of <code>xclim</code> merged in. This supersedes the BCSD downscaling service. (PR #98, PR #115, @dgergel)</li> </ul>"},{"location":"changelog/#050-2021-08-04","title":"0.5.0 - 2021-08-04","text":""},{"location":"changelog/#added_12","title":"Added","text":"<ul> <li>Add <code>--cyclic</code> option to regrid cli and services. (PR #108, @brews)</li> <li>Add <code>papermill</code>, <code>intake-esm</code> to Docker environment. (PR #106, @brews)</li> </ul>"},{"location":"changelog/#changed_13","title":"Changed","text":"<ul> <li>Bump environment <code>xarray</code> to v0.19.0. (PR #109, @brews)</li> </ul>"},{"location":"changelog/#041-2021-07-13","title":"0.4.1 - 2021-07-13","text":""},{"location":"changelog/#changed_14","title":"Changed","text":"<ul> <li>Bump xclim to v0.28.0, improve environment notes. (PR #105, @brews)</li> </ul>"},{"location":"changelog/#fixed_9","title":"Fixed","text":"<ul> <li>Fix application logging to stdout. (PR #104, @brews)</li> </ul>"},{"location":"changelog/#040-2021-07-09","title":"0.4.0 - 2021-07-09","text":""},{"location":"changelog/#added_13","title":"Added","text":"<ul> <li>Add <code>include-quantiles</code> flag to <code>apply_qdm</code> to allow for including quantile information in bias corrected output. (PR #95, @dgergel)</li> <li>Add precipitation unit conversion to <code>standardize_gcm</code>. (PR #94, @dgergel)</li> <li>Add <code>astype</code> argument to <code>regrid</code>. (PR #92, @brews)</li> </ul>"},{"location":"changelog/#changed_15","title":"Changed","text":"<ul> <li>Make <code>dodola</code> container's default CMD. (PR #90, @brews)</li> <li>Improve subprocess and death handling in Docker container. (PR #90, @brews)</li> </ul>"},{"location":"changelog/#fixed_10","title":"Fixed","text":"<ul> <li>Fix bug in train_quantiledeltamapping accounting for endpoints. (#87, @brews)</li> </ul>"},{"location":"changelog/#030-2021-06-16","title":"0.3.0 - 2021-06-16","text":""},{"location":"changelog/#added_14","title":"Added","text":"<ul> <li>Update <code>buildweights</code> service to add support for regridding to domain file. Not backwards compatible. (PR #67, @dgergel)</li> <li>Add downscaling service. Currently support BCSD spatial disaggregation as implemented in scikit-downscale. (PR #65, @dgergel)</li> <li>Add bias-correction quantile delta mapping (QDM) components to support Argo Workflows. New commands added: <code>dodola train-qdm</code> and <code>dodola apply-qdm</code>. (PR #70, @brews)</li> <li>Add wet day frequency correction service. Wet day frequency implemented as described in Cannon et al., 2015. New command added: <code>dodola correct-wetday-frequency</code>. (PR #78, @dgergel)</li> </ul>"},{"location":"changelog/#changed_16","title":"Changed","text":"<ul> <li>Significant updates to container environment: Python 3.9, <code>xarray</code>, <code>adlfs</code>, <code>xesmf</code>, <code>dask</code>, and <code>fsspec</code>. (PR #74, PR #75, PR #76, PR #77, PR #84 @brews)</li> <li>Remove stdout buffering from container runs, add IO debug logging. (PR #72, @brews)</li> </ul>"},{"location":"changelog/#fixed_11","title":"Fixed","text":"<ul> <li>Fix CMIP6 clean to better handle coords vs dims. (PR #81, @brews)</li> </ul>"},{"location":"changelog/#020-2021-04-23","title":"0.2.0 - 2021-04-23","text":""},{"location":"changelog/#changed_17","title":"Changed","text":"<ul> <li>Switch to pure <code>fsspec</code>-style URLs for data inputs. Added support for GCS buckets and S3 storage. Switch to <code>fsspec</code> backend settings to collect storage authentication. Because of this users likely will need to change the environment variables used to pass in storage credentials. <code>dodola.services</code> no longer require the <code>storage</code> argument. PR #61 from @brews.</li> <li>Switch to simple <code>xarray</code>-based rechunking to workaround to instability from our use of <code>rechunker</code>. This change breaks the CLI for <code>dodola rechunk</code>, removing the <code>-v/--variable</code> and <code>-m/--maxmemory</code> options. The change also breaks the <code>dodola.services.rechunk()</code> signature, removing the <code>max_mem</code> argument and the <code>target_chunks</code> argument is now a mapping <code>{coordinate_name: chunk_size}</code>. PR #60 from @brews.</li> </ul>"},{"location":"changelog/#fixed_12","title":"Fixed","text":"<ul> <li>Fix <code>TypeError</code> from <code>dodola rechunk</code>. PR #63 from @brews.</li> </ul>"},{"location":"changelog/#010-2021-04-15","title":"0.1.0 - 2021-04-15","text":"<ul> <li>Initial release.</li> </ul>"},{"location":"cli/","title":"dodola.cli","text":"<p>Commandline interface to the application.</p> <p>Functions:</p> Name Description <code>adjust_maximum_precipitation</code> <p>Apply maximum precipitation threshold to a dataset</p> <code>apply_dtr_floor</code> <p>Apply a floor to diurnal temperature range (DTR) in a dataset</p> <code>apply_non_polar_dtr_ceiling</code> <p>Apply a ceiling to diurnal temperature range (DTR) in a dataset</p> <code>apply_qdm</code> <p>Adjust simulation years with QDM bias-adjustment method, outputting Zarr Store</p> <code>apply_qplad</code> <p>Adjust simulation with QPLAD downscaling method, outputting Zarr Store</p> <code>cleancmip6</code> <p>Clean and standardize CMIP6 GCM to 'out'. If drop-leapdays option is set, remove leap days</p> <code>correct_wetday_frequency</code> <p>Correct wet day frequency in a dataset</p> <code>dodola_cli</code> <p>GCM bias adjustment and downscaling</p> <code>get_attrs</code> <p>Get JSON str of data attrs metadata.</p> <code>prime_qdm_output_zarrstore</code> <p>Initialize a Zarr Store for writing QDM output regionally in independent processes</p> <code>prime_qplad_output_zarrstore</code> <p>Initialize a Zarr Store for writing QPLAD output regionally in independent processes</p> <code>rechunk</code> <p>Rechunk Zarr store</p> <code>regrid</code> <p>Regrid a target climate dataset</p> <code>removeleapdays</code> <p>Remove leap days and update calendar attribute</p> <code>train_qdm</code> <p>Train Quantile Delta Mapping (QDM) model and output to storage</p> <code>train_qplad</code> <p>Train Quantile-Preserving, Localized Analogs Downscaling (QPLAD) model and output to storage</p> <code>validate_dataset</code> <p>Validate a dataset</p>"},{"location":"cli/#dodola.cli.adjust_maximum_precipitation","title":"dodola.cli.adjust_maximum_precipitation","text":"<pre><code>adjust_maximum_precipitation(x, out, threshold=3000.0)\n</code></pre> <p>Apply maximum precipitation threshold to a dataset</p> Source code in <code>dodola/cli.py</code> <pre><code>@dodola_cli.command(help=\"Adjust maximum precipitation in a dataset\")\n@click.argument(\"x\", required=True)\n@click.option(\"--out\", \"-o\", required=True)\n@click.option(\n    \"--threshold\", \"-t\", help=\"Threshold for correcting maximum precipitation\"\n)\ndef adjust_maximum_precipitation(x, out, threshold=3000.0):\n    \"\"\"Apply maximum precipitation threshold to a dataset\"\"\"\n    services.adjust_maximum_precipitation(\n        str(x), out=str(out), threshold=float(threshold)\n    )\n</code></pre>"},{"location":"cli/#dodola.cli.apply_dtr_floor","title":"dodola.cli.apply_dtr_floor","text":"<pre><code>apply_dtr_floor(x, out, floor=1.0)\n</code></pre> <p>Apply a floor to diurnal temperature range (DTR) in a dataset</p> Source code in <code>dodola/cli.py</code> <pre><code>@dodola_cli.command(\n    help=\"Apply a floor to diurnal temperature range (DTR) in a dataset\"\n)\n@click.argument(\"x\", required=True)\n@click.option(\"--out\", \"-o\", required=True)\n@click.option(\"--floor\", \"-f\", help=\"floor to apply to DTR values\")\ndef apply_dtr_floor(x, out, floor=1.0):\n    \"\"\"Apply a floor to diurnal temperature range (DTR) in a dataset\"\"\"\n    services.apply_dtr_floor(str(x), out=str(out), floor=float(floor))\n</code></pre>"},{"location":"cli/#dodola.cli.apply_non_polar_dtr_ceiling","title":"dodola.cli.apply_non_polar_dtr_ceiling","text":"<pre><code>apply_non_polar_dtr_ceiling(x, out, ceiling=70.0)\n</code></pre> <p>Apply a ceiling to diurnal temperature range (DTR) in a dataset</p> Source code in <code>dodola/cli.py</code> <pre><code>@dodola_cli.command(\n    help=\"Apply a ceiling to diurnal temperature range (DTR) in a dataset\"\n)\n@click.argument(\"x\", required=True)\n@click.option(\"--out\", \"-o\", required=True)\n@click.option(\"--ceiling\", \"-c\", help=\"ceiling to apply to DTR values\")\ndef apply_non_polar_dtr_ceiling(x, out, ceiling=70.0):\n    \"\"\"Apply a ceiling to diurnal temperature range (DTR) in a dataset\"\"\"\n    services.apply_non_polar_dtr_ceiling(str(x), out=str(out), ceiling=float(ceiling))\n</code></pre>"},{"location":"cli/#dodola.cli.apply_qdm","title":"dodola.cli.apply_qdm","text":"<pre><code>apply_qdm(simulation, qdm, years, variable, out, selslice=None, iselslice=None, out_zarr_region=None, root_attrs_json_file=None, new_attrs=None)\n</code></pre> <p>Adjust simulation years with QDM bias-adjustment method, outputting Zarr Store</p> Source code in <code>dodola/cli.py</code> <pre><code>@dodola_cli.command(help=\"Adjust simulation year with quantile delta mapping (QDM)\")\n@click.option(\n    \"--simulation\", \"-s\", required=True, help=\"URL to simulation store to adjust\"\n)\n@click.option(\"--qdm\", \"-q\", required=True, help=\"URL to trained QDM model store\")\n@click.option(\n    \"--years\",\n    required=True,\n    help=\"firstyear,lastyear inclusive range of years in simulation to adjust and output\",\n)\n@click.option(\"--variable\", \"-v\", required=True, help=\"Variable name in data stores\")\n@click.option(\n    \"--out\",\n    \"-o\",\n    required=True,\n    help=\"URL to write Zarr Store with adjusted simulation year to\",\n)\n@click.option(\n    \"--selslice\",\n    multiple=True,\n    required=False,\n    help=\"variable=start,stop to 'isel' slice input simulation before applying\",\n)\n@click.option(\n    \"--iselslice\",\n    multiple=True,\n    required=False,\n    help=\"variable=start,stop to 'sel' slice input simulation before applying\",\n)\n@click.option(\n    \"--out-zarr-region\",\n    multiple=True,\n    required=False,\n    help=\"variable=start,stop index to write output to region of existing Zarr Store\",\n)\n@click.option(\n    \"--root-attrs-json-file\",\n    help=\"fsspec-compatible URL pointing to a JSON file to use as root ``attrs`` for output data.\",\n)\n@click.option(\n    \"--new-attrs\",\n    multiple=True,\n    help=\"'key1=value1' entry to merge into the output Dataset root metadata (attrs)\",\n)\ndef apply_qdm(\n    simulation,\n    qdm,\n    years,\n    variable,\n    out,\n    selslice=None,\n    iselslice=None,\n    out_zarr_region=None,\n    root_attrs_json_file=None,\n    new_attrs=None,\n):\n    \"\"\"Adjust simulation years with QDM bias-adjustment method, outputting Zarr Store\"\"\"\n    first_year, last_year = (int(x) for x in years.split(\",\"))\n\n    unpacked_attrs = None\n    if new_attrs:\n        unpacked_attrs = {k: v for x in new_attrs for k, v in (x.split(\"=\"),)}\n\n    sel_slices_d = None\n    if selslice:\n        sel_slices_d = {}\n        for s in selslice:\n            k, v = s.split(\"=\")\n            sel_slices_d[k] = slice(*map(str, v.split(\",\")))\n\n    isel_slices_d = None\n    if iselslice:\n        isel_slices_d = {}\n        for s in iselslice:\n            k, v = s.split(\"=\")\n            isel_slices_d[k] = slice(*map(int, v.split(\",\")))\n\n    out_zarr_region_d = None\n    if out_zarr_region:\n        out_zarr_region_d = {}\n        for s in out_zarr_region:\n            k, v = s.split(\"=\")\n            out_zarr_region_d[k] = slice(*map(int, v.split(\",\")))\n\n    services.apply_qdm(\n        simulation=simulation,\n        qdm=qdm,\n        years=range(\n            first_year, last_year + 1\n        ),  # +1 because years is an inclusive range.\n        variable=variable,\n        out=out,\n        sel_slice=sel_slices_d,\n        isel_slice=isel_slices_d,\n        out_zarr_region=out_zarr_region_d,\n        root_attrs_json_file=root_attrs_json_file,\n        new_attrs=unpacked_attrs,\n    )\n</code></pre>"},{"location":"cli/#dodola.cli.apply_qplad","title":"dodola.cli.apply_qplad","text":"<pre><code>apply_qplad(simulation, qplad, variable, out, selslice=None, iselslice=None, out_zarr_region=None, root_attrs_json_file=None, new_attrs=None, wetday_post_correction=False)\n</code></pre> <p>Adjust simulation with QPLAD downscaling method, outputting Zarr Store</p> Source code in <code>dodola/cli.py</code> <pre><code>@dodola_cli.command(\n    help=\"Adjust (downscale) simulation year with Quantile-Preserving, Localized Analogs Downscaling (QPLAD)\"\n)\n@click.option(\n    \"--simulation\", \"-s\", required=True, help=\"URL to simulation store to adjust\"\n)\n@click.option(\n    \"--qplad\",\n    \"-d\",\n    required=True,\n    help=\"URL to trained QPLAD store of adjustment factors\",\n)\n@click.option(\"--variable\", \"-v\", required=True, help=\"Variable name in data stores\")\n@click.option(\n    \"--out\",\n    \"-o\",\n    required=True,\n    help=\"URL to write Zarr with downscaled output to\",\n)\n@click.option(\n    \"--selslice\",\n    multiple=True,\n    required=False,\n    help=\"variable=start,stop to 'isel' slice input simulation before applying\",\n)\n@click.option(\n    \"--iselslice\",\n    multiple=True,\n    required=False,\n    help=\"variable=start,stop to 'sel' slice input simulation before applying\",\n)\n@click.option(\n    \"--out-zarr-region\",\n    multiple=True,\n    required=False,\n    help=\"variable=start,stop index to write output to region of existing Zarr Store\",\n)\n@click.option(\n    \"--root-attrs-json-file\",\n    help=\"fsspec-compatible URL pointing to a JSON file to use as root ``attrs`` for output data.\",\n)\n@click.option(\n    \"--new-attrs\",\n    multiple=True,\n    help=\"'key1=value1' entry to merge into the output Dataset root metadata (attrs)\",\n)\n@click.option(\n    \"--wetday-post-correction\",\n    type=bool,\n    default=False,\n    help=\"Whether to apply wet day frequency adjustment on downscaled data\",\n)\ndef apply_qplad(\n    simulation,\n    qplad,\n    variable,\n    out,\n    selslice=None,\n    iselslice=None,\n    out_zarr_region=None,\n    root_attrs_json_file=None,\n    new_attrs=None,\n    wetday_post_correction=False,\n):\n    \"\"\"Adjust simulation with QPLAD downscaling method, outputting Zarr Store\"\"\"\n    unpacked_attrs = None\n    if new_attrs:\n        unpacked_attrs = {k: v for x in new_attrs for k, v in (x.split(\"=\"),)}\n\n    sel_slices_d = None\n    if selslice:\n        sel_slices_d = {}\n        for s in selslice:\n            k, v = s.split(\"=\")\n            sel_slices_d[k] = slice(*map(str, v.split(\",\")))\n\n    isel_slices_d = None\n    if iselslice:\n        isel_slices_d = {}\n        for s in iselslice:\n            k, v = s.split(\"=\")\n            isel_slices_d[k] = slice(*map(int, v.split(\",\")))\n\n    out_zarr_region_d = None\n    if out_zarr_region:\n        out_zarr_region_d = {}\n        for s in out_zarr_region:\n            k, v = s.split(\"=\")\n            out_zarr_region_d[k] = slice(*map(int, v.split(\",\")))\n\n    services.apply_qplad(\n        simulation=simulation,\n        qplad=qplad,\n        variable=variable,\n        out=out,\n        sel_slice=sel_slices_d,\n        isel_slice=isel_slices_d,\n        out_zarr_region=out_zarr_region_d,\n        root_attrs_json_file=root_attrs_json_file,\n        new_attrs=unpacked_attrs,\n        wet_day_post_correction=wetday_post_correction,\n    )\n</code></pre>"},{"location":"cli/#dodola.cli.cleancmip6","title":"dodola.cli.cleancmip6","text":"<pre><code>cleancmip6(x, out, drop_leapdays)\n</code></pre> <p>Clean and standardize CMIP6 GCM to 'out'. If drop-leapdays option is set, remove leap days</p> Source code in <code>dodola/cli.py</code> <pre><code>@dodola_cli.command(help=\"Clean up and standardize GCM\")\n@click.argument(\"x\", required=True)\n@click.argument(\"out\", required=True)\n@click.option(\n    \"--drop-leapdays/--no-drop-leapdays\",\n    default=True,\n    help=\"Whether to remove leap days\",\n)\ndef cleancmip6(x, out, drop_leapdays):\n    \"\"\"Clean and standardize CMIP6 GCM to 'out'. If drop-leapdays option is set, remove leap days\"\"\"\n    services.clean_cmip6(x, out, drop_leapdays)\n</code></pre>"},{"location":"cli/#dodola.cli.correct_wetday_frequency","title":"dodola.cli.correct_wetday_frequency","text":"<pre><code>correct_wetday_frequency(x, out, process)\n</code></pre> <p>Correct wet day frequency in a dataset</p> Source code in <code>dodola/cli.py</code> <pre><code>@dodola_cli.command(help=\"Correct wet day frequency in a dataset\")\n@click.argument(\"x\", required=True)\n@click.option(\"--out\", \"-o\", required=True)\n@click.option(\n    \"--process\",\n    \"-p\",\n    required=True,\n    type=click.Choice([\"pre\", \"post\"], case_sensitive=False),\n    help=\"Whether to pre or post process wet day frequency\",\n)\ndef correct_wetday_frequency(x, out, process):\n    \"\"\"Correct wet day frequency in a dataset\"\"\"\n    services.correct_wet_day_frequency(str(x), out=str(out), process=str(process))\n</code></pre>"},{"location":"cli/#dodola.cli.dodola_cli","title":"dodola.cli.dodola_cli","text":"<pre><code>dodola_cli(debug)\n</code></pre> <p>GCM bias adjustment and downscaling</p> <p>Authenticate with storage by setting the appropriate environment variables for your fsspec-compatible URL library.</p> Source code in <code>dodola/cli.py</code> <pre><code>@click.group(context_settings={\"help_option_names\": [\"-h\", \"--help\"]})\n@click.option(\"--debug/--no-debug\", default=False, envvar=\"DODOLA_DEBUG\")\ndef dodola_cli(debug):\n    \"\"\"GCM bias adjustment and downscaling\n\n    Authenticate with storage by setting the appropriate environment variables\n    for your fsspec-compatible URL library.\n    \"\"\"\n    noisy_loggers = [\n        \"azure.core.pipeline.policies.http_logging_policy\",\n        \"asyncio\",\n        \"adlfs.spec\",\n        \"gcsfs\",\n        \"chardet.universaldetector\",\n        \"fsspec\",\n    ]\n    for logger_name in noisy_loggers:\n        nl = logging.getLogger(logger_name)\n        nl.setLevel(logging.WARNING)\n\n    if debug:\n        logging.root.setLevel(logging.DEBUG)\n    else:\n        logging.root.setLevel(logging.INFO)\n</code></pre>"},{"location":"cli/#dodola.cli.get_attrs","title":"dodola.cli.get_attrs","text":"<pre><code>get_attrs(x, variable=None)\n</code></pre> <p>Get JSON str of data attrs metadata.</p> Source code in <code>dodola/cli.py</code> <pre><code>@dodola_cli.command(help=\"Get attrs from data\")\n@click.argument(\"x\", required=True)\n@click.option(\"--variable\", \"-v\", help=\"Variable name in data stores\")\ndef get_attrs(x, variable=None):\n    \"\"\"Get JSON str of data attrs metadata.\"\"\"\n    click.echo(services.get_attrs(x, variable))\n</code></pre>"},{"location":"cli/#dodola.cli.prime_qdm_output_zarrstore","title":"dodola.cli.prime_qdm_output_zarrstore","text":"<pre><code>prime_qdm_output_zarrstore(simulation, variable, years, out, zarr_region_dims=None, root_attrs_json_file=None, new_attrs=None)\n</code></pre> <p>Initialize a Zarr Store for writing QDM output regionally in independent processes</p> Source code in <code>dodola/cli.py</code> <pre><code>@dodola_cli.command(help=\"Prime a Zarr Store for regionally-written QDM output\")\n@click.option(\n    \"--simulation\", \"-s\", required=True, help=\"URL to simulation store to adjust\"\n)\n@click.option(\n    \"--years\",\n    required=True,\n    help=\"firstyear,lastyear inclusive range of years in simulation to adjust and output\",\n)\n@click.option(\"--variable\", \"-v\", required=True, help=\"Variable name in data stores\")\n@click.option(\n    \"--out\",\n    \"-o\",\n    required=True,\n    help=\"URL to write Zarr Store with adjusted simulation year to\",\n)\n@click.option(\n    \"--zarr-region-dims\",\n    required=True,\n    help=\"'variable1,variable2' comma-delimited list of variables used to define region when writing\",\n)\n@click.option(\n    \"--root-attrs-json-file\",\n    help=\"fsspec-compatible URL pointing to a JSON file to use as root ``attrs`` for output data.\",\n)\n@click.option(\n    \"--new-attrs\",\n    multiple=True,\n    help=\"'key1=value1' entry to merge into the output Dataset root metadata (attrs)\",\n)\ndef prime_qdm_output_zarrstore(\n    simulation,\n    variable,\n    years,\n    out,\n    zarr_region_dims=None,\n    root_attrs_json_file=None,\n    new_attrs=None,\n):\n    \"\"\"Initialize a Zarr Store for writing QDM output regionally in independent processes\"\"\"\n    first_year, last_year = (int(x) for x in years.split(\",\"))\n\n    unpacked_attrs = None\n    if new_attrs:\n        unpacked_attrs = {k: v for x in new_attrs for k, v in (x.split(\"=\"),)}\n\n    region_dims = zarr_region_dims.split(\",\")\n    services.prime_qdm_output_zarrstore(\n        simulation=simulation,\n        years=(first_year, last_year),\n        variable=variable,\n        out=out,\n        zarr_region_dims=region_dims,\n        root_attrs_json_file=root_attrs_json_file,\n        new_attrs=unpacked_attrs,\n    )\n</code></pre>"},{"location":"cli/#dodola.cli.prime_qplad_output_zarrstore","title":"dodola.cli.prime_qplad_output_zarrstore","text":"<pre><code>prime_qplad_output_zarrstore(simulation, variable, out, zarr_region_dims=None, root_attrs_json_file=None, new_attrs=None)\n</code></pre> <p>Initialize a Zarr Store for writing QPLAD output regionally in independent processes</p> Source code in <code>dodola/cli.py</code> <pre><code>@dodola_cli.command(help=\"Prime a Zarr Store for regionally-written QPLAD output\")\n@click.option(\n    \"--simulation\", \"-s\", required=True, help=\"URL to simulation store to adjust\"\n)\n@click.option(\"--variable\", \"-v\", required=True, help=\"Variable name in data stores\")\n@click.option(\n    \"--out\",\n    \"-o\",\n    required=True,\n    help=\"URL to write Zarr Store with adjusted simulation year to\",\n)\n@click.option(\n    \"--zarr-region-dims\",\n    required=True,\n    help=\"'variable1,variable2' comma-delimited list of variables used to define region when writing\",\n)\n@click.option(\n    \"--root-attrs-json-file\",\n    help=\"fsspec-compatible URL pointing to a JSON file to use as root ``attrs`` for output data.\",\n)\n@click.option(\n    \"--new-attrs\",\n    multiple=True,\n    help=\"'key1=value1' entry to merge into the output Dataset root metadata (attrs)\",\n)\ndef prime_qplad_output_zarrstore(\n    simulation,\n    variable,\n    out,\n    zarr_region_dims=None,\n    root_attrs_json_file=None,\n    new_attrs=None,\n):\n    \"\"\"Initialize a Zarr Store for writing QPLAD output regionally in independent processes\"\"\"\n    unpacked_attrs = None\n    if new_attrs:\n        unpacked_attrs = {k: v for x in new_attrs for k, v in (x.split(\"=\"),)}\n\n    region_dims = zarr_region_dims.split(\",\")\n    services.prime_qplad_output_zarrstore(\n        simulation=simulation,\n        variable=variable,\n        out=out,\n        zarr_region_dims=region_dims,\n        root_attrs_json_file=root_attrs_json_file,\n        new_attrs=unpacked_attrs,\n    )\n</code></pre>"},{"location":"cli/#dodola.cli.rechunk","title":"dodola.cli.rechunk","text":"<pre><code>rechunk(x, chunk, out)\n</code></pre> <p>Rechunk Zarr store</p> Source code in <code>dodola/cli.py</code> <pre><code>@dodola_cli.command(help=\"Rechunk Zarr store\")\n@click.argument(\"x\", required=True)\n@click.option(\n    \"--chunk\", \"-c\", multiple=True, required=True, help=\"coord=chunksize to rechunk to\"\n)\n@click.option(\"--out\", \"-o\", required=True)\ndef rechunk(x, chunk, out):\n    \"\"\"Rechunk Zarr store\"\"\"\n    # Convert [\"k1=1\", \"k2=2\"] into {k1: 1, k2: 2}\n    coord_chunks = {c.split(\"=\")[0]: int(c.split(\"=\")[1]) for c in chunk}\n\n    services.rechunk(\n        str(x),\n        target_chunks=coord_chunks,\n        out=out,\n    )\n</code></pre>"},{"location":"cli/#dodola.cli.regrid","title":"dodola.cli.regrid","text":"<pre><code>regrid(x, out, method, domain_file, weightspath, astype, cyclic)\n</code></pre> <p>Regrid a target climate dataset</p> <p>Note, the weightspath only accepts paths to NetCDF files on the local disk. See https://xesmf.readthedocs.io/ for details on requirements for <code>x</code> with different methods.</p> Source code in <code>dodola/cli.py</code> <pre><code>@dodola_cli.command(help=\"Build NetCDF weights file for regridding\")\n@click.argument(\"x\", required=True)\n@click.option(\"--out\", \"-o\", required=True)\n@click.option(\n    \"--method\",\n    \"-m\",\n    required=True,\n    help=\"Regridding method - 'bilinear' or 'conservative'\",\n)\n@click.option(\"--domain-file\", \"-d\", help=\"Domain file to regrid to\")\n@click.option(\n    \"--weightspath\",\n    \"-w\",\n    default=None,\n    help=\"Local path to existing regrid weights file\",\n)\n@click.option(\"--astype\", \"-t\", default=None, help=\"Type to recast output to\")\n@click.option(\n    \"--cyclic\", default=None, help=\"Add wrap-around values to dim before regridding\"\n)\ndef regrid(x, out, method, domain_file, weightspath, astype, cyclic):\n    \"\"\"Regrid a target climate dataset\n\n    Note, the weightspath only accepts paths to NetCDF files on the local disk. See\n    https://xesmf.readthedocs.io/ for details on requirements for `x` with\n    different methods.\n    \"\"\"\n    # Configure storage while we have access to users configurations.\n    services.regrid(\n        str(x),\n        out=str(out),\n        method=str(method),\n        domain_file=domain_file,\n        weights_path=weightspath,\n        astype=astype,\n        add_cyclic=cyclic,\n    )\n</code></pre>"},{"location":"cli/#dodola.cli.removeleapdays","title":"dodola.cli.removeleapdays","text":"<pre><code>removeleapdays(x, out)\n</code></pre> <p>Remove leap days and update calendar attribute</p> Source code in <code>dodola/cli.py</code> <pre><code>@dodola_cli.command(help=\"Remove leap days and update calendar\")\n@click.argument(\"x\", required=True)\n@click.argument(\"out\", required=True)\ndef removeleapdays(x, out):\n    \"\"\"Remove leap days and update calendar attribute\"\"\"\n    services.remove_leapdays(x, out)\n</code></pre>"},{"location":"cli/#dodola.cli.train_qdm","title":"dodola.cli.train_qdm","text":"<pre><code>train_qdm(historical, reference, out, variable, kind, selslice=None, iselslice=None)\n</code></pre> <p>Train Quantile Delta Mapping (QDM) model and output to storage</p> Source code in <code>dodola/cli.py</code> <pre><code>@dodola_cli.command(help=\"Train quantile delta mapping (QDM)\")\n@click.option(\n    \"--historical\", \"-h\", required=True, help=\"URL to historical simulation store\"\n)\n@click.option(\"--reference\", \"-r\", required=True, help=\"URL to reference data store\")\n@click.option(\"--variable\", \"-v\", required=True, help=\"Variable name in data stores\")\n@click.option(\n    \"--kind\",\n    \"-k\",\n    required=True,\n    type=click.Choice([\"additive\", \"multiplicative\"], case_sensitive=False),\n    help=\"Variable kind for mapping\",\n)\n@click.option(\"--out\", \"-o\", required=True, help=\"URL to write QDM model to\")\n@click.option(\n    \"--selslice\",\n    multiple=True,\n    required=False,\n    help=\"variable=start,stop to 'isel' slice inputs before training\",\n)\n@click.option(\n    \"--iselslice\",\n    multiple=True,\n    required=False,\n    help=\"variable=start,stop to 'sel' slice inputs before training\",\n)\ndef train_qdm(\n    historical, reference, out, variable, kind, selslice=None, iselslice=None\n):\n    \"\"\"Train Quantile Delta Mapping (QDM) model and output to storage\"\"\"\n    sel_slices_d = None\n    isel_slices_d = None\n\n    if selslice:\n        sel_slices_d = {}\n        for s in selslice:\n            k, v = s.split(\"=\")\n            sel_slices_d[k] = slice(*map(str, v.split(\",\")))\n\n    if iselslice:\n        isel_slices_d = {}\n        for s in iselslice:\n            k, v = s.split(\"=\")\n            isel_slices_d[k] = slice(*map(int, v.split(\",\")))\n\n    services.train_qdm(\n        historical=historical,\n        reference=reference,\n        out=out,\n        variable=variable,\n        kind=kind,\n        sel_slice=sel_slices_d,\n        isel_slice=isel_slices_d,\n    )\n</code></pre>"},{"location":"cli/#dodola.cli.train_qplad","title":"dodola.cli.train_qplad","text":"<pre><code>train_qplad(coarse_reference, fine_reference, out, variable, kind, selslice=None, iselslice=None)\n</code></pre> <p>Train Quantile-Preserving, Localized Analogs Downscaling (QPLAD) model and output to storage</p> Source code in <code>dodola/cli.py</code> <pre><code>@dodola_cli.command(\n    help=\"Train Quantile-Preserving, Localized Analogs Downscaling (QPLAD)\"\n)\n@click.option(\n    \"--coarse-reference\", \"-cr\", required=True, help=\"URL to coarse reference store\"\n)\n@click.option(\n    \"--fine-reference\", \"-fr\", required=True, help=\"URL to fine reference store\"\n)\n@click.option(\"--variable\", \"-v\", required=True, help=\"Variable name in data stores\")\n@click.option(\n    \"--kind\",\n    \"-k\",\n    required=True,\n    type=click.Choice([\"additive\", \"multiplicative\"], case_sensitive=False),\n    help=\"Variable kind for mapping\",\n)\n@click.option(\"--out\", \"-o\", required=True, help=\"URL to write QDM model to\")\n@click.option(\n    \"--selslice\",\n    multiple=True,\n    required=False,\n    help=\"variable=start,stop to 'isel' slice inputs before training\",\n)\n@click.option(\n    \"--iselslice\",\n    multiple=True,\n    required=False,\n    help=\"variable=start,stop to 'sel' slice inputs before training\",\n)\ndef train_qplad(\n    coarse_reference, fine_reference, out, variable, kind, selslice=None, iselslice=None\n):\n    \"\"\"Train Quantile-Preserving, Localized Analogs Downscaling (QPLAD) model and output to storage\"\"\"\n    sel_slices_d = None\n    isel_slices_d = None\n\n    if selslice:\n        sel_slices_d = {}\n        for s in selslice:\n            k, v = s.split(\"=\")\n            sel_slices_d[k] = slice(*map(str, v.split(\",\")))\n\n    if iselslice:\n        isel_slices_d = {}\n        for s in iselslice:\n            k, v = s.split(\"=\")\n            isel_slices_d[k] = slice(*map(int, v.split(\",\")))\n\n    services.train_qplad(\n        coarse_reference=coarse_reference,\n        fine_reference=fine_reference,\n        out=out,\n        variable=variable,\n        kind=kind,\n        sel_slice=sel_slices_d,\n        isel_slice=isel_slices_d,\n    )\n</code></pre>"},{"location":"cli/#dodola.cli.validate_dataset","title":"dodola.cli.validate_dataset","text":"<pre><code>validate_dataset(x, variable, data_type, time_period)\n</code></pre> <p>Validate a dataset</p> Source code in <code>dodola/cli.py</code> <pre><code>@dodola_cli.command(help=\"Validate a CMIP6, bias corrected or downscaled dataset\")\n@click.argument(\"x\", required=True)\n@click.option(\"--variable\", \"-v\", required=True)\n@click.option(\n    \"--data-type\",\n    \"-d\",\n    required=True,\n    type=click.Choice([\"cmip6\", \"bias_corrected\", \"downscaled\"], case_sensitive=False),\n    help=\"Which data type to validate\",\n)\n@click.option(\n    \"--time-period\",\n    \"-t\",\n    required=True,\n    type=click.Choice([\"historical\", \"future\"], case_sensitive=False),\n    help=\"Which time period to validate\",\n)\ndef validate_dataset(x, variable, data_type, time_period):\n    \"\"\"Validate a dataset\"\"\"\n    services.validate(\n        str(x),\n        var=str(variable),\n        data_type=str(data_type),\n        time_period=str(time_period),\n    )\n</code></pre>"},{"location":"core/","title":"dodola.core","text":"<p>Core logic for bias adjustment and downscaling</p> <p>Math stuff and business logic goes here. This is the \"business logic\".</p> <p>Functions:</p> Name Description <code>adjust_analogdownscaling</code> <p>Apply QPLAD to downscale bias corrected output.</p> <code>adjust_quantiledeltamapping</code> <p>Apply QDM to adjust a range of years within a simulation.</p> <code>adjust_quantiledeltamapping_year</code> <p>Apply QDM to adjust a year within a simulation.</p> <code>apply_precip_ceiling</code> <p>Converts all precip values above a threshold to the threshold value, uniformly across space and time.</p> <code>apply_wet_day_frequency_correction</code> <p>Parameters</p> <code>dtr_floor</code> <p>Converts all diurnal temperature range (DTR) values strictly below a floor</p> <code>non_polar_dtr_ceiling</code> <p>Converts all non-polar (regions between the 60th south and north parallel) diurnal temperature range (DTR) values strictly above a ceiling</p> <code>standardize_gcm</code> <p>360 calendar conversion requires that there are no chunks in</p> <code>test_dtr_range</code> <p>Ensure DTR values are in a valid range</p> <code>test_for_nans</code> <p>Tests for presence of NaNs</p> <code>test_maximum_precip</code> <p>Tests that max precip is reasonable</p> <code>test_negative_values</code> <p>Tests for presence of negative values</p> <code>test_temp_range</code> <p>Ensure temperature values are in a valid range</p> <code>test_timesteps</code> <p>Tests that Dataset contains the correct number of timesteps (number of days on a noleap calendar)</p> <code>test_variable_names</code> <p>Test that the correct variable name exists in the file</p> <code>train_analogdownscaling</code> <p>Train Quantile-Preserving, Localized Analogs Downscaling (QPLAD)</p> <code>train_quantiledeltamapping</code> <p>Train quantile delta mapping</p> <code>xclim_convert_360day_calendar_interpolate</code> <p>Parameters</p> <code>xclim_remove_leapdays</code> <p>Parameters</p> <code>xclim_units_any2pint</code> <p>Parameters</p> <code>xclim_units_pint2cf</code> <p>Parameters</p> <code>xesmf_regrid</code> <p>Regrid a Dataset.</p>"},{"location":"core/#dodola.core.adjust_analogdownscaling","title":"dodola.core.adjust_analogdownscaling","text":"<pre><code>adjust_analogdownscaling(simulation, qplad, variable)\n</code></pre> <p>Apply QPLAD to downscale bias corrected output.</p> <p>Parameters:</p> Name Type Description Default <code>simulation</code> <code>Dataset</code> <p>Daily bias corrected data to be downscaled. Target variable must have a units attribute.</p> required <code>qplad</code> <code>Dataset or QuantilePreservingAnalogDownscaling</code> <p>Trained <code>xclim.sdba.adjustment.QuantilePreservingAnalogDownscaling</code>, or Dataset representation that will instantiate <code>xclim.sdba.adjustment.QuantilePreservingAnalogDownscaling</code>.</p> required <code>variable</code> <code>str</code> <p>Target variable in <code>simulation</code> to downscale. Downscaled output will share the same name.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>Dataset</code> <p>QPLAD-downscaled values from <code>simulation</code>. May be a lazy-evaluated future, not yet computed.</p> Source code in <code>dodola/core.py</code> <pre><code>def adjust_analogdownscaling(simulation, qplad, variable):\n    \"\"\"Apply QPLAD to downscale bias corrected output.\n\n    Parameters\n    ----------\n    simulation : xr.Dataset\n        Daily bias corrected data to be downscaled. Target variable must have a units attribute.\n    qplad : xr.Dataset or sdba.adjustment.QuantilePreservingAnalogDownscaling\n        Trained ``xclim.sdba.adjustment.QuantilePreservingAnalogDownscaling``, or\n        Dataset representation that will instantiate\n        ``xclim.sdba.adjustment.QuantilePreservingAnalogDownscaling``.\n    variable : str\n        Target variable in `simulation` to downscale. Downscaled output will share the\n        same name.\n\n    Returns\n    -------\n    out : xr.Dataset\n        QPLAD-downscaled values from `simulation`. May be a lazy-evaluated future, not\n        yet computed.\n    \"\"\"\n    variable = str(variable)\n\n    if isinstance(qplad, xr.Dataset):\n        qplad = sdba.adjustment.QuantilePreservingAnalogDownscaling.from_dataset(qplad)\n\n    out = qplad.adjust(simulation[variable]).to_dataset(name=variable)\n\n    out = out.transpose(*simulation[variable].dims)\n    # Overwrite QPLAD output attrs with input simulation attrs.\n    out.attrs = simulation.attrs\n    for k, v in simulation.variables.items():\n        if k in out:\n            out[k].attrs = v.attrs\n\n    return out\n</code></pre>"},{"location":"core/#dodola.core.adjust_quantiledeltamapping","title":"dodola.core.adjust_quantiledeltamapping","text":"<pre><code>adjust_quantiledeltamapping(simulation, variable, qdm, years, astype=None, quantile_variable='sim_q', **kwargs)\n</code></pre> <p>Apply QDM to adjust a range of years within a simulation.</p> <p>Parameters:</p> Name Type Description Default <code>simulation</code> <code>Dataset</code> <p>Daily simulation data to be adjusted. Must have sufficient observations around <code>year</code> to adjust. Target variable must have a units attribute.</p> required <code>variable</code> <code>str</code> <p>Target variable in <code>simulation</code> to adjust. Adjusted output will share the same name.</p> required <code>qdm</code> <code>Dataset or QuantileDeltaMapping</code> <p>Trained <code>xclim.sdba.adjustment.QuantileDeltaMapping</code>, or Dataset representation that will be instantiate <code>xclim.sdba.adjustment.QuantileDeltaMapping</code>.</p> required <code>years</code> <code>sequence of ints</code> <p>Years of simulation to adjust, with rolling years and day grouping.</p> required <code>astype</code> <code>str, numpy.dtype, or None</code> <p>Typecode or data-type to which the regridded output is cast.</p> <code>None</code> <code>quantile_variable</code> <code>str or None</code> <p>Name of quantile coordinate to reset to data variable. Not reset if <code>None</code>.</p> <code>'sim_q'</code> <code>kwargs</code> <p>Keyword arguments passed to <code>dodola.core.adjust_quantiledeltamapping_year</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Dataset</code> <p>QDM-adjusted values from <code>simulation</code>. May be a lazy-evaluated future, not yet computed. In addition to adjusted original variables, this includes \"sim_q\" variable giving quantiles from QDM biasadjustment.</p> Source code in <code>dodola/core.py</code> <pre><code>def adjust_quantiledeltamapping(\n    simulation,\n    variable,\n    qdm,\n    years,\n    astype=None,\n    quantile_variable=\"sim_q\",\n    **kwargs,\n):\n    \"\"\"Apply QDM to adjust a range of years within a simulation.\n\n    Parameters\n    ----------\n    simulation : xr.Dataset\n        Daily simulation data to be adjusted. Must have sufficient observations\n        around `year` to adjust. Target variable must have a units attribute.\n    variable : str\n        Target variable in `simulation` to adjust. Adjusted output will\n        share the same name.\n    qdm : xr.Dataset or sdba.adjustment.QuantileDeltaMapping\n        Trained ``xclim.sdba.adjustment.QuantileDeltaMapping``, or Dataset\n        representation that will be instantiate\n        ``xclim.sdba.adjustment.QuantileDeltaMapping``.\n    years : sequence of ints\n        Years of simulation to adjust, with rolling years and day grouping.\n    astype : str, numpy.dtype, or None, optional\n        Typecode or data-type to which the regridded output is cast.\n    quantile_variable : str or None, optional\n        Name of quantile coordinate to reset to data variable. Not reset\n        if ``None``.\n    kwargs :\n        Keyword arguments passed to\n        ``dodola.core.adjust_quantiledeltamapping_year``.\n\n    Returns\n    -------\n    out : xr.Dataset\n        QDM-adjusted values from `simulation`. May be a lazy-evaluated future, not\n        yet computed. In addition to adjusted original variables, this includes\n        \"sim_q\" variable giving quantiles from QDM biasadjustment.\n    \"\"\"\n    # This loop is a candidate for dask.delayed. Beware, xclim had issues with saturated scheduler.\n    qdm_list = []\n    for yr in years:\n        adj = adjust_quantiledeltamapping_year(\n            simulation=simulation, qdm=qdm, year=yr, variable=variable, **kwargs\n        )\n        if astype:\n            adj = adj.astype(astype)\n        qdm_list.append(adj)\n\n    # Combine years and ensure output matches input data dimension order.\n    adjusted_ds = xr.concat(qdm_list, dim=\"time\").transpose(*simulation[variable].dims)\n\n    if quantile_variable:\n        adjusted_ds = adjusted_ds.reset_coords(quantile_variable)\n        # Analysts said sim_q needed no attrs.\n        adjusted_ds[quantile_variable].attrs = {}\n\n    # Overwrite QDM output attrs with input simulation attrs.\n    adjusted_ds.attrs = simulation.attrs\n    for k, v in simulation.variables.items():\n        if k in adjusted_ds:\n            adjusted_ds[k].attrs = v.attrs\n\n    return adjusted_ds\n</code></pre>"},{"location":"core/#dodola.core.adjust_quantiledeltamapping_year","title":"dodola.core.adjust_quantiledeltamapping_year","text":"<pre><code>adjust_quantiledeltamapping_year(simulation, qdm, year, variable, halfyearwindow_n=10, include_quantiles=False)\n</code></pre> <p>Apply QDM to adjust a year within a simulation.</p> <p>Parameters:</p> Name Type Description Default <code>simulation</code> <code>Dataset</code> <p>Daily simulation data to be adjusted. Must have sufficient observations around <code>year</code> to adjust. Target variable must have a units attribute.</p> required <code>qdm</code> <code>Dataset or QuantileDeltaMapping</code> <p>Trained <code>xclim.sdba.adjustment.QuantileDeltaMapping</code>, or Dataset representation that will be instantiate <code>xclim.sdba.adjustment.QuantileDeltaMapping</code>.</p> required <code>year</code> <code>int</code> <p>Target year to adjust, with rolling years and day grouping.</p> required <code>variable</code> <code>str</code> <p>Target variable in <code>simulation</code> to adjust. Adjusted output will share the same name.</p> required <code>halfyearwindow_n</code> <code>int</code> <p>Half-length of the annual rolling window to extract along either side of <code>year</code>.</p> <code>10</code> <code>include_quantiles</code> <code>bool</code> <p>Whether or not to output quantiles (sim_q) as a coordinate on the bias corrected data variable in output.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Dataset</code> <p>QDM-adjusted values from <code>simulation</code>. May be a lazy-evaluated future, not yet computed.</p> Source code in <code>dodola/core.py</code> <pre><code>def adjust_quantiledeltamapping_year(\n    simulation, qdm, year, variable, halfyearwindow_n=10, include_quantiles=False\n):\n    \"\"\"Apply QDM to adjust a year within a simulation.\n\n    Parameters\n    ----------\n    simulation : xr.Dataset\n        Daily simulation data to be adjusted. Must have sufficient observations\n        around `year` to adjust. Target variable must have a units attribute.\n    qdm : xr.Dataset or sdba.adjustment.QuantileDeltaMapping\n        Trained ``xclim.sdba.adjustment.QuantileDeltaMapping``, or\n        Dataset representation that will be instantiate\n        ``xclim.sdba.adjustment.QuantileDeltaMapping``.\n    year : int\n        Target year to adjust, with rolling years and day grouping.\n    variable : str\n        Target variable in `simulation` to adjust. Adjusted output will share the\n        same name.\n    halfyearwindow_n : int, optional\n        Half-length of the annual rolling window to extract along either\n        side of `year`.\n    include_quantiles : bool, optional\n        Whether or not to output quantiles (sim_q) as a coordinate on\n        the bias corrected data variable in output.\n\n    Returns\n    -------\n    out : xr.Dataset\n        QDM-adjusted values from `simulation`. May be a lazy-evaluated future, not\n        yet computed.\n    \"\"\"\n    year = int(year)\n    variable = str(variable)\n    halfyearwindow_n = int(halfyearwindow_n)\n\n    if isinstance(qdm, xr.Dataset):\n        qdm = sdba.adjustment.QuantileDeltaMapping.from_dataset(qdm)\n\n    # Slice to get 15 days before and after our target year. This accounts\n    # for the rolling 31 day rolling window.\n    timeslice = slice(\n        f\"{year - halfyearwindow_n - 1}-12-17\", f\"{year + halfyearwindow_n + 1}-01-15\"\n    )\n    simulation = simulation[variable].sel(\n        time=timeslice\n    )  # TODO: Need a check to ensure we have all the data in this slice!\n    if include_quantiles:\n        # include quantile information in output\n        with set_options(sdba_extra_output=True):\n            out = qdm.adjust(simulation, interp=\"nearest\").sel(time=str(year))\n            # make quantiles a coordinate of bias corrected output variable\n            out = out[\"scen\"].assign_coords(sim_q=out.sim_q)\n    else:\n        out = qdm.adjust(simulation, interp=\"nearest\").sel(time=str(year))\n\n    return out.to_dataset(name=variable)\n</code></pre>"},{"location":"core/#dodola.core.apply_precip_ceiling","title":"dodola.core.apply_precip_ceiling","text":"<pre><code>apply_precip_ceiling(ds, ceiling)\n</code></pre> <p>Converts all precip values above a threshold to the threshold value, uniformly across space and time.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> required <code>ceiling</code> <code>int or float</code> required <p>Returns:</p> Type Description <code>Dataset</code> Source code in <code>dodola/core.py</code> <pre><code>def apply_precip_ceiling(ds, ceiling):\n    \"\"\"\n    Converts all precip values above a threshold to the threshold value, uniformly across space and time.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n    ceiling : int or float\n\n    Returns\n    -------\n    xr.Dataset\n\n    \"\"\"\n    ds_corrected = ds.where(ds &lt;= ceiling, ceiling)\n    return ds_corrected\n</code></pre>"},{"location":"core/#dodola.core.apply_wet_day_frequency_correction","title":"dodola.core.apply_wet_day_frequency_correction","text":"<pre><code>apply_wet_day_frequency_correction(ds, process, variable='pr')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> required <code>process</code> <code>(pre, post)</code> <code>\"pre\"</code> <code>variable</code> <code>'pr'</code> <p>Returns:</p> Type Description <code>Dataset</code> Notes <p>[1] A.J. Cannon, S.R. Sobie, and T.Q. Murdock (2015), \"Bias correction of GCM     precipitation by quantile mapping: How well do methods preserve     changes in quantiles and extremes?\", Journal of Climate, vol.     28, Issue 7, pp. 6938-6959. [2] S. Hempel, K. Frieler, L. Warszawski, J. Schewe, and F. Piotek (2013), \"A trend-preserving bias correction - The ISI-MIP approach\", Earth Syst. Dynam. vol. 4, pp. 219-236.</p> Source code in <code>dodola/core.py</code> <pre><code>def apply_wet_day_frequency_correction(ds, process, variable=\"pr\"):\n    \"\"\"\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n    process : {\"pre\", \"post\"}\n    variable: str\n\n    Returns\n    -------\n    xr.Dataset\n\n    Notes\n    -------\n    [1] A.J. Cannon, S.R. Sobie, and T.Q. Murdock (2015), \"Bias correction of GCM\n        precipitation by quantile mapping: How well do methods preserve\n        changes in quantiles and extremes?\", Journal of Climate, vol.\n        28, Issue 7, pp. 6938-6959.\n    [2] S. Hempel, K. Frieler, L. Warszawski, J. Schewe, and F. Piotek (2013), \"A trend-preserving bias correction - The ISI-MIP approach\", Earth Syst. Dynam. vol. 4, pp. 219-236.\n    \"\"\"\n    # threshold from Hempel et al 2013\n    threshold = 1.0  # mm/day\n    # adjusted \"low\" value from the original epsilon in Cannon et al 2015 to\n    # avoid having some values get extremely large\n    low = threshold / 2.0\n\n    if process == \"pre\":\n        # includes very small values that are negative in CMIP6 output\n        ds[variable] = ds[variable].where(\n            ds[variable] &gt;= threshold,\n            np.random.uniform(\n                low=low,\n                high=threshold,\n                size=ds[variable].shape,\n            ).astype(ds[variable].data.dtype),\n        )\n    elif process == \"post\":\n        ds[variable] = ds[variable].where(ds[variable] &gt;= threshold, 0.0)\n    else:\n        raise ValueError(\"this processing option is not implemented\")\n    return ds\n</code></pre>"},{"location":"core/#dodola.core.dtr_floor","title":"dodola.core.dtr_floor","text":"<pre><code>dtr_floor(ds, floor)\n</code></pre> <p>Converts all diurnal temperature range (DTR) values strictly below a floor to that floor.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> required <code>floor</code> <code>int or float</code> required <p>Returns:</p> Type Description <code>Dataset</code> Source code in <code>dodola/core.py</code> <pre><code>def dtr_floor(ds, floor):\n    \"\"\"\n    Converts all diurnal temperature range (DTR) values strictly below a floor\n    to that floor.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n    floor : int or float\n\n    Returns\n    -------\n    xr.Dataset\n\n    \"\"\"\n\n    ds_corrected = ds.where(ds &gt;= floor, floor)\n    return ds_corrected\n</code></pre>"},{"location":"core/#dodola.core.non_polar_dtr_ceiling","title":"dodola.core.non_polar_dtr_ceiling","text":"<pre><code>non_polar_dtr_ceiling(ds, ceiling)\n</code></pre> <p>Converts all non-polar (regions between the 60th south and north parallel) diurnal temperature range (DTR) values strictly above a ceiling to that ceiling.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> required <code>ceiling</code> <code>int or float</code> required <p>Returns:</p> Type Description <code>Dataset</code> Source code in <code>dodola/core.py</code> <pre><code>def non_polar_dtr_ceiling(ds, ceiling):\n    \"\"\"\n    Converts all non-polar (regions between the 60th south and north parallel) diurnal temperature range (DTR) values strictly above a ceiling\n    to that ceiling.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n    ceiling : int or float\n\n    Returns\n    -------\n    xr.Dataset\n\n    \"\"\"\n\n    ds_corrected = ds.where(\n        xr.ufuncs.logical_or(\n            ds &lt;= ceiling, xr.ufuncs.logical_or(ds[\"lat\"] &lt;= -60, ds[\"lat\"] &gt;= 60)\n        ),\n        ceiling,\n    )\n\n    return ds_corrected\n</code></pre>"},{"location":"core/#dodola.core.standardize_gcm","title":"dodola.core.standardize_gcm","text":"<pre><code>standardize_gcm(ds, leapday_removal=True)\n</code></pre> <p>360 calendar conversion requires that there are no chunks in the 'time' dimension of <code>ds</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> required <code>leapday_removal</code> <code>bool</code> <code>True</code> <p>Returns:</p> Type Description <code>Dataset</code> Source code in <code>dodola/core.py</code> <pre><code>def standardize_gcm(ds, leapday_removal=True):\n    \"\"\"\n\n    360 calendar conversion requires that there are no chunks in\n    the 'time' dimension of `ds`.\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n    leapday_removal : bool, optional\n\n    Returns\n    -------\n    xr.Dataset\n    \"\"\"\n    # Remove cruft coordinates, variables, dims.\n    cruft_vars = (\"height\", \"member_id\", \"time_bnds\")\n\n    dims_to_squeeze = []\n    coords_to_drop = []\n    for v in cruft_vars:\n        if v in ds.dims:\n            dims_to_squeeze.append(v)\n        elif v in ds.coords:\n            coords_to_drop.append(v)\n\n    ds_cleaned = ds.squeeze(dims_to_squeeze, drop=True).reset_coords(\n        coords_to_drop, drop=True\n    )\n\n    # Cleanup time.\n\n    # if variable is precip, need to update units to mm day-1\n    if \"pr\" in ds_cleaned.variables:\n        # units should be kg/m2/s in CMIP6 output\n        if ds_cleaned[\"pr\"].units == \"kg m-2 s-1\":\n            # convert to mm/day\n            mmday_conversion = 24 * 60 * 60\n            ds_cleaned[\"pr\"] = ds_cleaned[\"pr\"] * mmday_conversion\n            # update units attribute\n            ds_cleaned[\"pr\"].attrs[\"units\"] = \"mm day-1\"\n        else:\n            # we want this to fail, as pr units are something we don't expect\n            raise ValueError(\"check units: pr units attribute is not kg m-2 s-1\")\n\n    cal = get_calendar(ds_cleaned)\n\n    if (\n        cal == \"360_day\" or leapday_removal\n    ):  # calendar conversion is necessary in either case\n        # if calendar is just integers, xclim cannot understand it\n        if ds_cleaned.time.dtype == \"int64\":\n            ds_cleaned[\"time\"] = xr.decode_cf(ds_cleaned).time\n        if cal == \"360_day\":\n            # Cannot have chunks in time dimension for 360 day calendar conversion so loading\n            # data into memory.\n            ds_cleaned.load()\n\n            if leapday_removal:  # 360 day -&gt; noleap\n                ds_converted = xclim_convert_360day_calendar_interpolate(\n                    ds=ds_cleaned,\n                    target=\"noleap\",\n                    align_on=\"random\",\n                    interpolation=\"linear\",\n                )\n            else:  # 360 day -&gt; standard\n                ds_converted = xclim_convert_360day_calendar_interpolate(\n                    ds=ds_cleaned,\n                    target=\"standard\",\n                    align_on=\"random\",\n                    interpolation=\"linear\",\n                )\n        else:  # any -&gt; noleap\n            # remove leap days and update calendar\n            ds_converted = xclim_remove_leapdays(ds_cleaned)\n\n        # rechunk, otherwise chunks are different sizes\n        ds_out = ds_converted.chunk(\n            {\"time\": 730, \"lat\": len(ds_cleaned.lat), \"lon\": len(ds_cleaned.lon)}\n        )\n\n    else:\n        ds_out = ds_cleaned\n\n    return ds_out\n</code></pre>"},{"location":"core/#dodola.core.test_dtr_range","title":"dodola.core.test_dtr_range","text":"<pre><code>test_dtr_range(ds, var, data_type)\n</code></pre> <p>Ensure DTR values are in a valid range Test polar values separately since some polar values can be much higher post bias adjustment.</p> Source code in <code>dodola/core.py</code> <pre><code>def test_dtr_range(ds, var, data_type):\n    \"\"\"\n    Ensure DTR values are in a valid range\n    Test polar values separately since some polar values can be much higher post bias adjustment.\n    \"\"\"\n    # test that DTR values are greater than 0 or equal to 0 depending on the data type\n    # note that some CMIP6 DTR will equal 0 at polar latitudes, this will be adjusted\n    # before bias adjustment with the DTR small values correction\n\n    dtr_min = ds[var].min()\n    if data_type == \"cmip6\":\n        # may be equal to zero in polar regions and if tasmax &lt; tasmin (only occurs for GFDL models)\n        assert (\n            dtr_min &gt;= 0\n        ), \"diurnal temperature range minimum is {} and thus not greater than or equal to 0 for CMIP6\".format(\n            dtr_min\n        )\n    else:\n        # this must be greater than 0 for bias corrected and downscaled\n        assert (\n            dtr_min &gt; 0\n        ), \"diurnal temperature range minimum is {} and must be greater than zero\".format(\n            dtr_min\n        )\n\n    # test polar DTR values\n    southern_polar_max = ds[var].where(ds.lat &lt; -60).max()\n    if (southern_polar_max is not None) and (southern_polar_max &gt;= 100):\n        assert (\n            southern_polar_max &lt; 100\n        ), \"diurnal temperature range max is {} for polar southern latitudes\".format(\n            southern_polar_max\n        )\n\n    northern_polar_max = ds[var].where(ds.lat &gt; 60).max()\n    if (northern_polar_max is not None) and (northern_polar_max &gt;= 100):\n        assert (\n            northern_polar_max &lt; 100\n        ), \"diurnal temperature range max is {} for polar northern latitudes\".format(\n            northern_polar_max\n        )\n\n    # test all but polar regions\n    non_polar_max = ds[var].where((ds.lat &gt; -60) &amp; (ds.lat &lt; 60)).max()\n    assert (\n        non_polar_max &lt;= 70\n    ), \"diurnal temperature range max is {} for non-polar regions\".format(non_polar_max)\n</code></pre>"},{"location":"core/#dodola.core.test_for_nans","title":"dodola.core.test_for_nans","text":"<pre><code>test_for_nans(ds, var)\n</code></pre> <p>Tests for presence of NaNs</p> Source code in <code>dodola/core.py</code> <pre><code>def test_for_nans(ds, var):\n    \"\"\"\n    Tests for presence of NaNs\n    \"\"\"\n    assert ds[var].isnull().sum() == 0, \"there are nans!\"\n</code></pre>"},{"location":"core/#dodola.core.test_maximum_precip","title":"dodola.core.test_maximum_precip","text":"<pre><code>test_maximum_precip(ds, var)\n</code></pre> <p>Tests that max precip is reasonable</p> Source code in <code>dodola/core.py</code> <pre><code>def test_maximum_precip(ds, var):\n    \"\"\"\n    Tests that max precip is reasonable\n    \"\"\"\n    threshold = 3000  # in mm, max observed is 1.825m --&gt; maximum occurs between 0.5-0.8\n    max_precip = ds[var].max().load().values\n    num_precip_values_over_threshold = (\n        ds[var].where(ds[var] &gt; threshold).count().load().values\n    )\n    assert (\n        num_precip_values_over_threshold == 0\n    ), \"maximum precip is {} mm and there are {} values over 3000mm\".format(\n        max_precip, num_precip_values_over_threshold\n    )\n</code></pre>"},{"location":"core/#dodola.core.test_negative_values","title":"dodola.core.test_negative_values","text":"<pre><code>test_negative_values(ds, var)\n</code></pre> <p>Tests for presence of negative values</p> Source code in <code>dodola/core.py</code> <pre><code>def test_negative_values(ds, var):\n    \"\"\"\n    Tests for presence of negative values\n    \"\"\"\n    # this is not set to 0 to deal with floating point error\n    neg_values = ds[var].where(ds[var] &lt; -0.001).count()\n    assert neg_values == 0, \"there are {} negative values!\".format(neg_values)\n</code></pre>"},{"location":"core/#dodola.core.test_temp_range","title":"dodola.core.test_temp_range","text":"<pre><code>test_temp_range(ds, var)\n</code></pre> <p>Ensure temperature values are in a valid range</p> Source code in <code>dodola/core.py</code> <pre><code>def test_temp_range(ds, var):\n    \"\"\"\n    Ensure temperature values are in a valid range\n    \"\"\"\n    # This high 377 K temperature range is to allow UKESM1-0-LL runs, which\n    # apparently run very hot.\n    assert (ds[var].min() &gt; 130) and (\n        ds[var].max() &lt; 377\n    ), \"{} values are invalid\".format(var)\n</code></pre>"},{"location":"core/#dodola.core.test_timesteps","title":"dodola.core.test_timesteps","text":"<pre><code>test_timesteps(ds, data_type, time_period)\n</code></pre> <p>Tests that Dataset contains the correct number of timesteps (number of days on a noleap calendar) for the data_type/time_period combination.</p> Source code in <code>dodola/core.py</code> <pre><code>def test_timesteps(ds, data_type, time_period):\n    \"\"\"\n    Tests that Dataset contains the correct number of timesteps (number of days on a noleap calendar)\n    for the data_type/time_period combination.\n    \"\"\"\n    if time_period == \"future\":\n        # bias corrected/downscaled data has 2015 - 2099 or 2100 depending on the model\n        # CMIP6 future data has an additional ten years from the historical model run\n        if data_type == \"cmip6\":\n            assert (\n                len(ds.time) &gt;= 35040  # some CMIP6 data ends in 2099\n            ), \"projection {} file is missing timesteps, has {}\".format(\n                data_type, len(ds.time)\n            )\n            if len(ds.time) &gt; 35405:\n                warnings.warn(\n                    \"projection {} file has excess timesteps, has {}\".format(\n                        data_type, len(ds.time)\n                    )\n                )\n        else:\n            assert (\n                len(ds.time) &gt;= 31025  # 2015 - 2099\n            ), \"projection {} file is missing timesteps, has {}\".format(\n                data_type, len(ds.time)\n            )\n            if len(ds.time) &gt; 31390:  # 2015 - 2100\n                warnings.warn(\n                    \"projection {} file has excess timesteps, has {}\".format(\n                        data_type, len(ds.time)\n                    )\n                )\n\n    elif time_period == \"historical\":\n        # bias corrected/downscaled data should have 1950 - 2014\n        # CMIP6 historical data has an additional ten years from SSP 370 (or 245 if 370 not available)\n        if data_type == \"cmip6\":\n            assert (\n                len(ds.time) &gt;= 27740\n            ), \"historical {} file is missing timesteps, has {}\".format(\n                data_type, len(ds.time)\n            )\n            if len(ds.time) &gt; 27740:\n                warnings.warn(\n                    \"historical {} file has excess timesteps, has {}\".format(\n                        data_type, len(ds.time)\n                    )\n                )\n        else:\n            assert (\n                len(ds.time) &gt;= 23725\n            ), \"historical {} file is missing timesteps, has {}\".format(\n                data_type, len(ds.time)\n            )\n            if len(ds.time) &gt; 23725:\n                warnings.warn(\n                    \"historical {} file has excess timesteps, has {}\".format(\n                        data_type, len(ds.time)\n                    )\n                )\n</code></pre>"},{"location":"core/#dodola.core.test_variable_names","title":"dodola.core.test_variable_names","text":"<pre><code>test_variable_names(ds, var)\n</code></pre> <p>Test that the correct variable name exists in the file</p> Source code in <code>dodola/core.py</code> <pre><code>def test_variable_names(ds, var):\n    \"\"\"\n    Test that the correct variable name exists in the file\n    \"\"\"\n    assert var in ds.var(), \"{} not in Dataset\".format(var)\n</code></pre>"},{"location":"core/#dodola.core.train_analogdownscaling","title":"dodola.core.train_analogdownscaling","text":"<pre><code>train_analogdownscaling(coarse_reference, fine_reference, variable, kind, quantiles_n=620, window_n=31)\n</code></pre> <p>Train Quantile-Preserving, Localized Analogs Downscaling (QPLAD)</p> <p>Parameters:</p> Name Type Description Default <code>coarse_reference</code> <code>Dataset</code> <p>Dataset to use as resampled (to fine resolution) coarse reference.Target variable must have a units attribute.</p> required <code>fine_reference</code> <code>Dataset</code> <p>Dataset to use as fine-resolution reference. Target variable must have a units attribute.</p> required <code>variable</code> <code>str</code> <p>Name of target variable to extract from <code>coarse_reference</code> and <code>fine_reference</code>.</p> required <code>kind</code> <code>('+', '*')</code> <p>Kind of variable. Used for creating QPLAD adjustment factors.</p> <code>\"+\"</code> <code>quantiles_n</code> <code>int</code> <p>Number of quantiles for QPLAD.</p> <code>620</code> <code>window_n</code> <code>int</code> <p>Centered window size for day-of-year grouping.</p> <code>31</code> <p>Returns:</p> Type Description <code>QuantilePreservingAnalogDownscaling</code> Source code in <code>dodola/core.py</code> <pre><code>def train_analogdownscaling(\n    coarse_reference, fine_reference, variable, kind, quantiles_n=620, window_n=31\n):\n    \"\"\"Train Quantile-Preserving, Localized Analogs Downscaling (QPLAD)\n\n    Parameters\n    ----------\n    coarse_reference : xr.Dataset\n        Dataset to use as resampled (to fine resolution) coarse reference.Target variable must have a units attribute.\n    fine_reference : xr.Dataset\n        Dataset to use as fine-resolution reference. Target variable must have a units attribute.\n    variable : str\n        Name of target variable to extract from `coarse_reference` and `fine_reference`.\n    kind : {\"+\", \"*\"}\n        Kind of variable. Used for creating QPLAD adjustment factors.\n    quantiles_n : int, optional\n        Number of quantiles for QPLAD.\n    window_n : int, optional\n        Centered window size for day-of-year grouping.\n\n    Returns\n    -------\n    xclim.sdba.adjustment.QuantilePreservingAnalogDownscaling\n    \"\"\"\n\n    # QPLAD method requires that the number of quantiles equals\n    # the number of days in each day group\n    # e.g. 20 years of data and a window of 31 = 620 quantiles\n\n    # check that lengths of input data are the same, then only check years for one\n    if len(coarse_reference.time) != len(fine_reference.time):\n        raise ValueError(\"coarse and fine reference data inputs have different lengths\")\n\n    # check number of years in input data (subtract 2 for the +/- 15 days on each end)\n    num_years = len(np.unique(fine_reference.time.dt.year)) - 2\n    if (num_years * int(window_n)) != quantiles_n:\n        raise ValueError(\n            \"number of quantiles {} must equal # of years {} * window length {}, day groups must {} days\".format(\n                quantiles_n, num_years, int(window_n), quantiles_n\n            )\n        )\n\n    qplad = sdba.adjustment.QuantilePreservingAnalogDownscaling.train(\n        ref=coarse_reference[variable],\n        hist=fine_reference[variable],\n        kind=str(kind),\n        group=sdba.Grouper(\"time.dayofyear\", window=int(window_n)),\n        nquantiles=quantiles_n,\n    )\n    return qplad\n</code></pre>"},{"location":"core/#dodola.core.train_quantiledeltamapping","title":"dodola.core.train_quantiledeltamapping","text":"<pre><code>train_quantiledeltamapping(reference, historical, variable, kind, quantiles_n=100, window_n=31)\n</code></pre> <p>Train quantile delta mapping</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>Dataset</code> <p>Dataset to use as model reference. Target variable must have a units attribute.</p> required <code>historical</code> <code>Dataset</code> <p>Dataset to use as historical simulation. Target variable must have a units attribute.</p> required <code>variable</code> <code>str</code> <p>Name of target variable to extract from <code>historical</code> and <code>reference</code>.</p> required <code>kind</code> <code>('+', '*')</code> <p>Kind of variable. Used for QDM scaling.</p> <code>\"+\"</code> <code>quantiles_n</code> <code>int</code> <p>Number of quantiles for QDM.</p> <code>100</code> <code>window_n</code> <code>int</code> <p>Centered window size for day-of-year grouping.</p> <code>31</code> <p>Returns:</p> Type Description <code>QuantileDeltaMapping</code> Source code in <code>dodola/core.py</code> <pre><code>def train_quantiledeltamapping(\n    reference, historical, variable, kind, quantiles_n=100, window_n=31\n):\n    \"\"\"Train quantile delta mapping\n\n    Parameters\n    ----------\n    reference : xr.Dataset\n        Dataset to use as model reference. Target variable must have a units attribute.\n    historical : xr.Dataset\n        Dataset to use as historical simulation. Target variable must have a units attribute.\n    variable : str\n        Name of target variable to extract from `historical` and `reference`.\n    kind : {\"+\", \"*\"}\n        Kind of variable. Used for QDM scaling.\n    quantiles_n : int, optional\n        Number of quantiles for QDM.\n    window_n : int, optional\n        Centered window size for day-of-year grouping.\n\n    Returns\n    -------\n    xclim.sdba.adjustment.QuantileDeltaMapping\n    \"\"\"\n    qdm = sdba.adjustment.QuantileDeltaMapping.train(\n        ref=reference[variable],\n        hist=historical[variable],\n        kind=str(kind),\n        group=sdba.Grouper(\"time.dayofyear\", window=int(window_n)),\n        nquantiles=equally_spaced_nodes(int(quantiles_n), eps=None),\n    )\n    return qdm\n</code></pre>"},{"location":"core/#dodola.core.xclim_convert_360day_calendar_interpolate","title":"dodola.core.xclim_convert_360day_calendar_interpolate","text":"<pre><code>xclim_convert_360day_calendar_interpolate(ds, target='noleap', align_on='random', interpolation='linear', return_indices=False, ignore_nans=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> required <code>target</code> <code>str</code> <p>see xclim.core.calendar.convert_calendar</p> <code>'noleap'</code> <code>align_on</code> <code>str</code> <p>this determines which days in the calendar will have missing values or will be the product of interpolation, if there is. It could be every year the same calendar days, or the days could randomly change. see xclim.core.calendar.convert_calendar</p> <code>'random'</code> <code>interpolation</code> <code>None or str</code> <p>passed to xr.Dataset.interpolate_na if not None</p> <code>'linear'</code> <code>return_indices</code> <code>bool</code> <p>on top of the converted dataset, return a list of the array indices identifying values that were inserted. This assumes there were no NaNs before conversion.</p> <code>False</code> <code>ignore_nans</code> <code>bool</code> <p>if False and there are any NaNs in <code>ds</code> variables, an assertion error will be raised. NaNs are ignored otherwise.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple(xr.Dataset, xr.Dataset) if return_indices is True, xr.Dataset otherwise.</code> Notes <p>The default values of <code>target</code>, <code>align_on</code> and <code>interpolation</code> mean that our default approach is equivalent to that of the LOCA calendar conversion [1] for conversion from 360 days calendars to noleap calendars. In that approach, 5 calendar days are added (noleap calendars always have 365 days) to each year. But those calendar days are not necessarily those that will have their value be the product of interpolation. The days for which we interpolate are selected randomly every block of 72 days, so that they change every year.</p> <p>[1] http://loca.ucsd.edu/loca-calendar/</p> Source code in <code>dodola/core.py</code> <pre><code>def xclim_convert_360day_calendar_interpolate(\n    ds,\n    target=\"noleap\",\n    align_on=\"random\",\n    interpolation=\"linear\",\n    return_indices=False,\n    ignore_nans=True,\n):\n    \"\"\"\n    Parameters\n    ----------\n    ds : xr.Dataset\n    target : str\n        see xclim.core.calendar.convert_calendar\n    align_on : str\n        this determines which days in the calendar will have missing values or will be the product of interpolation, if there is.\n        It could be every year the same calendar days, or the days could randomly change. see xclim.core.calendar.convert_calendar\n    interpolation : None or str\n        passed to xr.Dataset.interpolate_na if not None\n    return_indices : bool\n        on top of the converted dataset, return a list of the array indices identifying values that were inserted.\n        This assumes there were no NaNs before conversion.\n    ignore_nans : bool\n        if False and there are any NaNs in `ds` variables, an assertion error will be raised. NaNs are ignored otherwise.\n    Returns\n    -------\n    tuple(xr.Dataset, xr.Dataset) if return_indices is True, xr.Dataset otherwise.\n\n    Notes\n    -----\n    The default values of `target`, `align_on` and `interpolation` mean that our default approach is equivalent to that of the LOCA\n    calendar conversion [1] for conversion from 360 days calendars to noleap calendars. In that approach, 5 calendar days are added (noleap\n    calendars always have 365 days) to each year. But those calendar days are not necessarily those that will have their value be the product\n    of interpolation. The days for which we interpolate are selected randomly every block of 72 days, so that they change every year.\n\n    [1] http://loca.ucsd.edu/loca-calendar/\n    \"\"\"\n\n    if get_calendar(ds) != \"360_day\":\n        raise ValueError(\n            \"tried to use 360 day calendar conversion for a non-360-day calendar dataset\"\n        )\n\n    if not ignore_nans:\n        for var in ds:\n            assert (\n                ds[var].isnull().sum() == 0\n            ), \"360 days calendar conversion with interpolation : there are nans !\"\n\n    ds_converted = convert_calendar(\n        ds, target=target, align_on=align_on, missing=np.NaN\n    )\n\n    if interpolation:\n        ds_out = ds_converted.interpolate_na(\"time\", interpolation)\n    else:\n        ds_out = ds_converted\n\n    if return_indices:\n        return (ds_out, xr.ufuncs.isnan(ds_converted))\n    else:\n        return ds_out\n</code></pre>"},{"location":"core/#dodola.core.xclim_remove_leapdays","title":"dodola.core.xclim_remove_leapdays","text":"<pre><code>xclim_remove_leapdays(ds)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> required <p>Returns:</p> Type Description <code>Dataset</code> Source code in <code>dodola/core.py</code> <pre><code>def xclim_remove_leapdays(ds):\n    \"\"\"\n\n    Parameters\n    ----------\n    ds : xr.Dataset\n\n    Returns\n    -------\n    xr.Dataset\n    \"\"\"\n    ds_noleap = convert_calendar(ds, target=\"noleap\")\n    return ds_noleap\n</code></pre>"},{"location":"core/#dodola.core.xclim_units_any2pint","title":"dodola.core.xclim_units_any2pint","text":"<pre><code>xclim_units_any2pint(ds, var)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> required <code>var</code> <code>str</code> required <p>Returns:</p> Type Description <code>xr.Dataset with `var` units str attribute converted to xclim's pint registry format</code> Source code in <code>dodola/core.py</code> <pre><code>def xclim_units_any2pint(ds, var):\n    \"\"\"\n    Parameters\n    ----------\n    ds : xr.Dataset\n    var : str\n\n    Returns\n    -------\n    xr.Dataset with `var` units str attribute converted to xclim's pint registry format\n    \"\"\"\n\n    logger.info(f\"Reformatting {var} unit string representation\")\n    ds[var].attrs[\"units\"] = str(xclim_units.units2pint(ds[var].attrs[\"units\"]))\n    return ds\n</code></pre>"},{"location":"core/#dodola.core.xclim_units_pint2cf","title":"dodola.core.xclim_units_pint2cf","text":"<pre><code>xclim_units_pint2cf(ds, var)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> required <code>var</code> <code>str</code> required <p>Returns:</p> Type Description <code>xr.Dataset with `var` units str attribute converted to CF format</code> Source code in <code>dodola/core.py</code> <pre><code>def xclim_units_pint2cf(ds, var):\n    \"\"\"\n    Parameters\n    ----------\n    ds : xr.Dataset\n    var : str\n\n    Returns\n    -------\n    xr.Dataset with `var` units str attribute converted to CF format\n    \"\"\"\n    logger.info(f\"Reformatting {var} unit string representation\")\n    ds[var].attrs[\"units\"] = xclim_units.pint2cfunits(\n        xclim_units.units2pint(ds[var].attrs[\"units\"])\n    )\n    return ds\n</code></pre>"},{"location":"core/#dodola.core.xesmf_regrid","title":"dodola.core.xesmf_regrid","text":"<pre><code>xesmf_regrid(x, domain, method, weights_path=None, astype=None, add_cyclic=None, keep_attrs=True)\n</code></pre> <p>Regrid a Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Dataset</code> required <code>domain</code> <code>Dataset</code> <p>Domain to regrid to.</p> required <code>method</code> <code>str</code> <p>Method of regridding. Passed to <code>xesmf.Regridder</code>.</p> required <code>weights_path</code> <code>str</code> <p>Local path to netCDF file of pre-calculated XESMF regridding weights.</p> <code>None</code> <code>astype</code> <code>str, numpy.dtype, or None</code> <p>Typecode or data-type to which the regridded output is cast.</p> <code>None</code> <code>add_cyclic</code> <code>str, or None</code> <p>Add cyclic point (aka wrap-around pixel) to given dimension before regridding. Useful for avoiding dateline artifacts along longitude in global datasets.</p> <code>None</code> <code>keep_attrs</code> <code>bool</code> <p>Whether to pass attrs from input to regridded output.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dataset</code> Source code in <code>dodola/core.py</code> <pre><code>def xesmf_regrid(\n    x, domain, method, weights_path=None, astype=None, add_cyclic=None, keep_attrs=True\n):\n    \"\"\"\n    Regrid a Dataset.\n\n    Parameters\n    ----------\n    x : xr.Dataset\n    domain : xr.Dataset\n        Domain to regrid to.\n    method : str\n        Method of regridding. Passed to ``xesmf.Regridder``.\n    weights_path : str, optional\n        Local path to netCDF file of pre-calculated XESMF regridding weights.\n    astype : str, numpy.dtype, or None, optional\n        Typecode or data-type to which the regridded output is cast.\n    add_cyclic : str, or None, optional\n        Add cyclic point (aka wrap-around pixel) to given dimension before\n        regridding. Useful for avoiding dateline artifacts along longitude\n        in global datasets.\n    keep_attrs : bool, optional\n        Whether to pass attrs from input to regridded output.\n\n    Returns\n    -------\n    xr.Dataset\n    \"\"\"\n    if add_cyclic:\n        x = _add_cyclic(x, add_cyclic)\n\n    regridder = xe.Regridder(\n        x,\n        domain,\n        method=method,\n        filename=weights_path,\n    )\n    if astype:\n        return regridder(x, keep_attrs=keep_attrs).astype(astype)\n    return regridder(x, keep_attrs=keep_attrs)\n</code></pre>"},{"location":"repository/","title":"dodola.repository","text":"<p>Objects to read and write stored climate model data.</p> <p>Functions:</p> Name Description <code>read</code> <p>Read Dataset from Zarr store</p> <code>read_attrs</code> <p>Read and deserialize JSON attrs file</p> <code>write</code> <p>Write Dataset to Zarr Store</p>"},{"location":"repository/#dodola.repository.read","title":"dodola.repository.read","text":"<pre><code>read(url_or_path)\n</code></pre> <p>Read Dataset from Zarr store</p> <p>Parameters:</p> Name Type Description Default <code>url_or_path</code> <code>str</code> <p>Location of Zarr store to read.</p> required <p>Returns:</p> Type Description <code>Dataset</code> Source code in <code>dodola/repository.py</code> <pre><code>def read(url_or_path):\n    \"\"\"Read Dataset from Zarr store\n\n    Parameters\n    ----------\n    url_or_path : str\n        Location of Zarr store to read.\n\n    Returns\n    -------\n    xr.Dataset\n    \"\"\"\n    logger.debug(f\"Reading {url_or_path}\")\n    x = open_zarr(url_or_path)\n    logger.info(f\"Read {url_or_path}\")\n    return x\n</code></pre>"},{"location":"repository/#dodola.repository.read_attrs","title":"dodola.repository.read_attrs","text":"<pre><code>read_attrs(urlpath)\n</code></pre> <p>Read and deserialize JSON attrs file</p> Source code in <code>dodola/repository.py</code> <pre><code>def read_attrs(urlpath):\n    \"\"\"Read and deserialize JSON attrs file\"\"\"\n    logger.debug(f\"Reading attrs from {urlpath}\")\n\n    with fsspec.open(urlpath) as f:\n        out = json.load(f)\n        logger.info(f\"Read attrs from {urlpath}\")\n\n    logger.debug(f\"Read attrs {out}\")\n    return out\n</code></pre>"},{"location":"repository/#dodola.repository.write","title":"dodola.repository.write","text":"<pre><code>write(url_or_path, x, region=None)\n</code></pre> <p>Write Dataset to Zarr Store</p> <p>Note, any lazy computations will be evaluated.</p> <p>Parameters:</p> Name Type Description Default <code>url_or_path</code> <code>str</code> <p>Location to write Zarr store to.</p> required <code>x</code> <code>Dataset</code> required <code>region</code> <code>dict or None</code> <p>Optional mapping from dimension names to integer slices along dataset dimensions to indicate the region of existing zarr array(s) in which to write this dataset\u2019s data. Variables not sliced in the region are dropped.</p> <code>None</code> Source code in <code>dodola/repository.py</code> <pre><code>def write(url_or_path, x, region=None):\n    \"\"\"Write Dataset to Zarr Store\n\n    Note, any lazy computations will be evaluated.\n\n    Parameters\n    ----------\n    url_or_path : str\n        Location to write Zarr store to.\n    x : xr.Dataset\n    region : dict or None, optional\n        Optional mapping from dimension names to integer slices along dataset\n        dimensions to indicate the region of existing zarr array(s) in\n        which to write this dataset\u2019s data. Variables not sliced in the region\n        are dropped.\n    \"\"\"\n    logger.debug(f\"Writing {url_or_path}\")\n    logger.debug(f\"Output Dataset {x=}\")\n\n    if region:\n        # TODO: This behavior needs a better, focused, unit test.\n        logger.info(f\"Writing to Zarr Store region, {region=}\")\n\n        # We need to drop all variables not sliced by the selected zarr_region.\n        variables_to_drop = []\n        region_variables = list(region.keys())\n        for variable_name, variable in x.variables.items():\n            if any(\n                region_variable not in variable.dims\n                for region_variable in region_variables\n            ):\n                variables_to_drop.append(variable_name)\n\n        logger.info(\n            f\"Dropping variables before Zarr region write: {variables_to_drop=}\"\n        )\n        x = x.drop_vars(variables_to_drop)\n\n        x.to_zarr(url_or_path, region=region, mode=\"a\", compute=True)\n    else:\n        x.to_zarr(url_or_path, mode=\"w\", compute=True)\n    logger.info(f\"Written {url_or_path}\")\n</code></pre>"},{"location":"services/","title":"dodola.services","text":"<p>Used by the CLI or any UI to deliver services to our lovely users</p> <p>Functions:</p> Name Description <code>adjust_maximum_precipitation</code> <p>Adjusts maximum precipitation in a dataset</p> <code>apply_dtr_floor</code> <p>Applies a floor to diurnal temperature range (DTR) values</p> <code>apply_non_polar_dtr_ceiling</code> <p>Applies a ceiling to diurnal temperature range (DTR) values</p> <code>apply_qdm</code> <p>Apply trained QDM to adjust a years in a simulation, write to Zarr Store.</p> <code>apply_qplad</code> <p>Apply QPLAD adjustment factors to downscale a simulation, dump to NetCDF.</p> <code>clean_cmip6</code> <p>Cleans and standardizes CMIP6 GCM</p> <code>correct_wet_day_frequency</code> <p>Corrects wet day frequency in a dataset</p> <code>get_attrs</code> <p>Get JSON str of <code>x</code> attrs metadata.</p> <code>log_service</code> <p>Decorator for dodola.services to log service start and stop</p> <code>prime_qdm_output_zarrstore</code> <p>Init a Zarr Store for writing QDM output regionally in independent processes.</p> <code>prime_qplad_output_zarrstore</code> <p>Init a Zarr Store for writing QPLAD output regionally in independent processes.</p> <code>rechunk</code> <p>Rechunk data to specification</p> <code>regrid</code> <p>Regrid climate data</p> <code>remove_leapdays</code> <p>Removes leap days and updates calendar attribute</p> <code>train_qdm</code> <p>Train quantile delta mapping and dump to <code>out</code></p> <code>train_qplad</code> <p>Train Quantile-Preserving, Localized Analogs Downscaling and dump to <code>out</code></p> <code>validate</code> <p>Performs validation on an input dataset</p>"},{"location":"services/#dodola.services.adjust_maximum_precipitation","title":"dodola.services.adjust_maximum_precipitation","text":"<pre><code>adjust_maximum_precipitation(x, out, threshold=3000.0)\n</code></pre> <p>Adjusts maximum precipitation in a dataset</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>str</code> <p>Storage URL to input xr.Dataset that will be corrected.</p> required <code>out</code> <code>str</code> <p>Storage URL to write corrected output to.</p> required <code>threshold</code> <code>int or float</code> <p>All precipitation values lower than this value are corrected to the threshold value.</p> <code>3000.0</code> Source code in <code>dodola/services.py</code> <pre><code>@log_service\ndef adjust_maximum_precipitation(x, out, threshold=3000.0):\n    \"\"\"Adjusts maximum precipitation in a dataset\n\n    Parameters\n    ----------\n    x : str\n        Storage URL to input xr.Dataset that will be corrected.\n    out : str\n        Storage URL to write corrected output to.\n    threshold : int or float, optional\n        All precipitation values lower than this value are corrected to the threshold value.\n    \"\"\"\n\n    ds = storage.read(x)\n    ds_corrected = apply_precip_ceiling(ds, threshold)\n    storage.write(out, ds_corrected)\n</code></pre>"},{"location":"services/#dodola.services.apply_dtr_floor","title":"dodola.services.apply_dtr_floor","text":"<pre><code>apply_dtr_floor(x, out, floor=1.0)\n</code></pre> <p>Applies a floor to diurnal temperature range (DTR) values</p> <p>This constrains the values in a DTR dataset by applying a floor. The floor is assigned to the value of the data points which have their value strictly below the floor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>str</code> <p>Storage URL to input xr.Dataset that will be corrected.</p> required <code>out</code> <code>str</code> <p>Storage URL to write corrected output to.</p> required <code>floor</code> <code>int or float</code> <p>All DTR values lower than this value are corrected to that value.</p> <code>1.0</code> Source code in <code>dodola/services.py</code> <pre><code>@log_service\ndef apply_dtr_floor(x, out, floor=1.0):\n    \"\"\"Applies a floor to diurnal temperature range (DTR) values\n\n    This constrains the values in a DTR dataset by applying a floor. The floor is assigned to the value of the\n    data points which have their value strictly below the floor.\n\n    Parameters\n    ----------\n    x : str\n        Storage URL to input xr.Dataset that will be corrected.\n    out : str\n        Storage URL to write corrected output to.\n    floor : int or float, optional\n        All DTR values lower than this value are corrected to that value.\n    \"\"\"\n    ds = storage.read(x)\n    ds = dtr_floor(ds, floor)\n    storage.write(out, ds)\n</code></pre>"},{"location":"services/#dodola.services.apply_non_polar_dtr_ceiling","title":"dodola.services.apply_non_polar_dtr_ceiling","text":"<pre><code>apply_non_polar_dtr_ceiling(x, out, ceiling=70.0)\n</code></pre> <p>Applies a ceiling to diurnal temperature range (DTR) values</p> <p>This constrains the values in a DTR dataset by applying a ceiling. The ceiling is assigned to the value of the data points which have their value strictly above the ceiling.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>str</code> <p>Storage URL to input xr.Dataset that will be corrected.</p> required <code>out</code> <code>str</code> <p>Storage URL to write corrected output to.</p> required <code>ceiling</code> <code>int or float</code> <p>All DTR values above this value are corrected to that value.</p> <code>70.0</code> Source code in <code>dodola/services.py</code> <pre><code>@log_service\ndef apply_non_polar_dtr_ceiling(x, out, ceiling=70.0):\n    \"\"\"Applies a ceiling to diurnal temperature range (DTR) values\n\n    This constrains the values in a DTR dataset by applying a ceiling. The ceiling is assigned to the value of the\n    data points which have their value strictly above the ceiling.\n\n    Parameters\n    ----------\n    x : str\n        Storage URL to input xr.Dataset that will be corrected.\n    out : str\n        Storage URL to write corrected output to.\n    ceiling : int or float, optional\n        All DTR values above this value are corrected to that value.\n    \"\"\"\n    ds = storage.read(x)\n    ds = non_polar_dtr_ceiling(ds, ceiling)\n    storage.write(out, ds)\n</code></pre>"},{"location":"services/#dodola.services.apply_qdm","title":"dodola.services.apply_qdm","text":"<pre><code>apply_qdm(simulation, qdm, years, variable, out, sel_slice=None, isel_slice=None, out_zarr_region=None, root_attrs_json_file=None, new_attrs=None)\n</code></pre> <p>Apply trained QDM to adjust a years in a simulation, write to Zarr Store.</p> <p>Output includes bias-corrected variable <code>variable</code> as well as a variable giving quantiles from the QDM, \"sim_q\".</p> <p>Parameters:</p> Name Type Description Default <code>simulation</code> <code>str</code> <p>fsspec-compatible URL containing simulation data to be adjusted.</p> required <code>qdm</code> <code>str</code> <p>fsspec-compatible URL pointing to Zarr Store containing canned <code>xclim.sdba.adjustment.QuantileDeltaMapping</code> Dataset.</p> required <code>years</code> <code>sequence of ints</code> <p>Years of simulation to adjust, with rolling years and day grouping.</p> required <code>variable</code> <code>str</code> <p>Target variable in <code>simulation</code> to adjust. Adjusted output will share the same name.</p> required <code>out</code> <code>str</code> <p>fsspec-compatible path or URL pointing to Zarr Store file where the QDM-adjusted simulation data will be written.</p> required <code>sel_slice</code> <p>Label-index slice input slimulation dataset before adjusting. A mapping of {variable_name: slice(...)} passed to <code>xarray.Dataset.sel()</code>.</p> <code>None</code> <code>isel_slice</code> <p>Integer-index slice input slimulation dataset before adjusting. A mapping of {variable_name: slice(...)} passed to <code>xarray.Dataset.isel()</code>.</p> <code>None</code> <code>out_zarr_region</code> <p>A mapping of {variable_name: slice(...)} giving the region to write to if outputting to existing Zarr Store.</p> <code>None</code> <code>root_attrs_json_file</code> <code>str or None</code> <p>fsspec-compatible URL pointing to a JSON file to use as root <code>attrs</code> for the output data. <code>new_attrs</code> will be appended to this.</p> <code>None</code> <code>new_attrs</code> <code>dict or None</code> <p>dict to merge with output Dataset's root <code>attrs</code> before output.</p> <code>None</code> Source code in <code>dodola/services.py</code> <pre><code>@log_service\ndef apply_qdm(\n    simulation,\n    qdm,\n    years,\n    variable,\n    out,\n    sel_slice=None,\n    isel_slice=None,\n    out_zarr_region=None,\n    root_attrs_json_file=None,\n    new_attrs=None,\n):\n    \"\"\"Apply trained QDM to adjust a years in a simulation, write to Zarr Store.\n\n    Output includes bias-corrected variable `variable` as well as a variable giving quantiles\n    from the QDM, \"sim_q\".\n\n    Parameters\n    ----------\n    simulation : str\n        fsspec-compatible URL containing simulation data to be adjusted.\n    qdm : str\n        fsspec-compatible URL pointing to Zarr Store containing canned\n        ``xclim.sdba.adjustment.QuantileDeltaMapping`` Dataset.\n    years : sequence of ints\n        Years of simulation to adjust, with rolling years and day grouping.\n    variable : str\n        Target variable in `simulation` to adjust. Adjusted output will share the\n        same name.\n    out : str\n        fsspec-compatible path or URL pointing to Zarr Store file where the\n        QDM-adjusted simulation data will be written.\n    sel_slice: dict or None, optional\n        Label-index slice input slimulation dataset before adjusting.\n        A mapping of {variable_name: slice(...)} passed to\n        `xarray.Dataset.sel()`.\n    isel_slice: dict or None, optional\n        Integer-index slice input slimulation dataset before adjusting. A mapping\n        of {variable_name: slice(...)} passed to `xarray.Dataset.isel()`.\n    out_zarr_region: dict or None, optional\n        A mapping of {variable_name: slice(...)} giving the region to write\n        to if outputting to existing Zarr Store.\n    root_attrs_json_file : str or None, optional\n        fsspec-compatible URL pointing to a JSON file to use as root ``attrs``\n        for the output data. ``new_attrs`` will be appended to this.\n    new_attrs : dict or None, optional\n        dict to merge with output Dataset's root ``attrs`` before output.\n    \"\"\"\n    sim_ds = storage.read(simulation)\n    qdm_ds = storage.read(qdm)\n\n    if root_attrs_json_file:\n        logger.info(f\"Using root attrs from {root_attrs_json_file}\")\n        sim_ds.attrs = storage.read_attrs(root_attrs_json_file)\n\n    if sel_slice:\n        logger.info(f\"Slicing by {sel_slice=}\")\n        sim_ds = sim_ds.sel(sel_slice)\n\n    if isel_slice:\n        logger.info(f\"Slicing by {isel_slice=}\")\n        sim_ds = sim_ds.isel(isel_slice)\n\n    variable = str(variable)\n\n    qdm_ds.load()\n    sim_ds.load()\n\n    sim_ds = xclim_units_any2pint(sim_ds, variable)\n\n    adjusted_ds = adjust_quantiledeltamapping(\n        simulation=sim_ds,\n        variable=variable,\n        qdm=qdm_ds,\n        years=years,\n        astype=sim_ds[variable].dtype,\n        include_quantiles=True,\n    )\n\n    adjusted_ds = xclim_units_pint2cf(adjusted_ds, variable)\n\n    if new_attrs:\n        adjusted_ds.attrs |= new_attrs\n\n    storage.write(out, adjusted_ds, region=out_zarr_region)\n</code></pre>"},{"location":"services/#dodola.services.apply_qplad","title":"dodola.services.apply_qplad","text":"<pre><code>apply_qplad(simulation, qplad, variable, out, sel_slice=None, isel_slice=None, out_zarr_region=None, root_attrs_json_file=None, new_attrs=None, wet_day_post_correction=False)\n</code></pre> <p>Apply QPLAD adjustment factors to downscale a simulation, dump to NetCDF.</p> <p>Dumping to NetCDF is a feature likely to change in the near future.</p> <p>Parameters:</p> Name Type Description Default <code>simulation</code> <code>str</code> <p>fsspec-compatible URL containing simulation data to be adjusted. Dataset must have <code>variable</code> as well as a variable, \"sim_q\", giving the quantiles from QDM bias adjustment.</p> required <code>qplad</code> <code>str</code> <p>fsspec-compatible URL pointing to Zarr Store containing canned <code>xclim.sdba.adjustment.AnalogQuantilePreservingDownscaling</code> Dataset.</p> required <code>variable</code> <code>str</code> <p>Target variable in <code>simulation</code> to downscale. Downscaled output will share the same name.</p> required <code>out</code> <code>str</code> <p>fsspec-compatible path or URL pointing to Zarr Store where the QPLAD-downscaled simulation data will be written.</p> required <code>sel_slice</code> <p>Label-index slice input slimulation dataset before adjusting. A mapping of {variable_name: slice(...)} passed to <code>xarray.Dataset.sel()</code>.</p> <code>None</code> <code>isel_slice</code> <p>Integer-index slice input slimulation dataset before adjusting. A mapping of {variable_name: slice(...)} passed to <code>xarray.Dataset.isel()</code>.</p> <code>None</code> <code>out_zarr_region</code> <p>A mapping of {variable_name: slice(...)} giving the region to write to if outputting to existing Zarr Store.</p> <code>None</code> <code>root_attrs_json_file</code> <code>str or None</code> <p>fsspec-compatible URL pointing to a JSON file to use as root <code>attrs</code> for the output data. <code>new_attrs</code> will be appended to this.</p> <code>None</code> <code>new_attrs</code> <code>dict or None</code> <p>dict to merge with output Dataset's root <code>attrs</code> before output.</p> <code>None</code> <code>wet_day_post_correction</code> <code>bool</code> <p>Whether to apply wet day frequency correction on downscaled data</p> <code>False</code> Source code in <code>dodola/services.py</code> <pre><code>@log_service\ndef apply_qplad(\n    simulation,\n    qplad,\n    variable,\n    out,\n    sel_slice=None,\n    isel_slice=None,\n    out_zarr_region=None,\n    root_attrs_json_file=None,\n    new_attrs=None,\n    wet_day_post_correction=False,\n):\n    \"\"\"Apply QPLAD adjustment factors to downscale a simulation, dump to NetCDF.\n\n    Dumping to NetCDF is a feature likely to change in the near future.\n\n    Parameters\n    ----------\n    simulation : str\n        fsspec-compatible URL containing simulation data to be adjusted.\n        Dataset must have `variable` as well as a variable, \"sim_q\", giving\n        the quantiles from QDM bias adjustment.\n    qplad : str\n        fsspec-compatible URL pointing to Zarr Store containing canned\n        ``xclim.sdba.adjustment.AnalogQuantilePreservingDownscaling`` Dataset.\n    variable : str\n        Target variable in `simulation` to downscale. Downscaled output will share the\n        same name.\n    out : str\n        fsspec-compatible path or URL pointing to Zarr Store where the\n        QPLAD-downscaled simulation data will be written.\n    sel_slice: dict or None, optional\n        Label-index slice input slimulation dataset before adjusting.\n        A mapping of {variable_name: slice(...)} passed to\n        `xarray.Dataset.sel()`.\n    isel_slice: dict or None, optional\n        Integer-index slice input slimulation dataset before adjusting. A mapping\n        of {variable_name: slice(...)} passed to `xarray.Dataset.isel()`.\n    out_zarr_region: dict or None, optional\n        A mapping of {variable_name: slice(...)} giving the region to write\n        to if outputting to existing Zarr Store.\n    root_attrs_json_file : str or None, optional\n        fsspec-compatible URL pointing to a JSON file to use as root ``attrs``\n        for the output data. ``new_attrs`` will be appended to this.\n    new_attrs : dict or None, optional\n        dict to merge with output Dataset's root ``attrs`` before output.\n    wet_day_post_correction : bool\n        Whether to apply wet day frequency correction on downscaled data\n    \"\"\"\n    sim_ds = storage.read(simulation)\n    qplad_ds = storage.read(qplad)\n\n    if root_attrs_json_file:\n        logger.info(f\"Using root attrs from {root_attrs_json_file}\")\n        sim_ds.attrs = storage.read_attrs(root_attrs_json_file)\n\n    if sel_slice:\n        logger.info(f\"Slicing by {sel_slice=}\")\n        sim_ds = sim_ds.sel(sel_slice)\n\n    if isel_slice:\n        logger.info(f\"Slicing by {isel_slice=}\")\n        sim_ds = sim_ds.isel(isel_slice)\n\n    sim_ds = sim_ds.set_coords([\"sim_q\"])\n\n    # needs to not be chunked\n    sim_ds = sim_ds.load()\n    qplad_ds = qplad_ds.load()\n\n    variable = str(variable)\n\n    sim_ds = xclim_units_any2pint(sim_ds, variable)\n    for var in qplad_ds:\n        qplad_ds = xclim_units_any2pint(qplad_ds, var)\n\n    adjusted_ds = adjust_analogdownscaling(\n        simulation=sim_ds, qplad=qplad_ds, variable=variable\n    )\n\n    # no need to revert units change. adjust_analogdownscaling does it\n\n    if wet_day_post_correction:\n        adjusted_ds = apply_wet_day_frequency_correction(adjusted_ds, \"post\")\n\n    if new_attrs:\n        adjusted_ds.attrs |= new_attrs\n\n    storage.write(out, adjusted_ds, region=out_zarr_region)\n</code></pre>"},{"location":"services/#dodola.services.clean_cmip6","title":"dodola.services.clean_cmip6","text":"<pre><code>clean_cmip6(x, out, leapday_removal)\n</code></pre> <p>Cleans and standardizes CMIP6 GCM</p> <p>This loads the entire <code>x</code> Dataset into memory for speed and to avoid chunking errors.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>str</code> <p>Storage URL to input xr.Dataset that will be cleaned.</p> required <code>out</code> <code>str</code> <p>Storage URL to write cleaned GCM output to.</p> required <code>leapday_removal</code> <code>bool</code> <p>Whether or not to remove leap days.</p> required Source code in <code>dodola/services.py</code> <pre><code>@log_service\ndef clean_cmip6(x, out, leapday_removal):\n    \"\"\"Cleans and standardizes CMIP6 GCM\n\n    This loads the entire `x` Dataset into memory for speed\n    and to avoid chunking errors.\n\n    Parameters\n    ----------\n    x : str\n        Storage URL to input xr.Dataset that will be cleaned.\n    out : str\n        Storage URL to write cleaned GCM output to.\n    leapday_removal : bool\n        Whether or not to remove leap days.\n    \"\"\"\n    ds = storage.read(x)\n\n    cleaned_ds = standardize_gcm(ds, leapday_removal)\n    storage.write(out, cleaned_ds)\n</code></pre>"},{"location":"services/#dodola.services.correct_wet_day_frequency","title":"dodola.services.correct_wet_day_frequency","text":"<pre><code>correct_wet_day_frequency(x, out, process, variable='pr')\n</code></pre> <p>Corrects wet day frequency in a dataset</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>str</code> <p>Storage URL to input xr.Dataset that will be regridded.</p> required <code>out</code> <code>str</code> <p>Storage URL to write regridded output to.</p> required <code>process</code> <code>(pre, post)</code> <p>Step in pipeline, used in determining how to correct. \"Pre\" replaces all zero values with a uniform random value below a threshold (before bias adjustment). \"Post\" replaces all values below a threshold with zeroes (after bias adjustment).</p> <code>\"pre\"</code> <code>variable</code> <code>'pr'</code> Source code in <code>dodola/services.py</code> <pre><code>@log_service\ndef correct_wet_day_frequency(x, out, process, variable=\"pr\"):\n    \"\"\"Corrects wet day frequency in a dataset\n\n    Parameters\n    ----------\n    x : str\n        Storage URL to input xr.Dataset that will be regridded.\n    out : str\n        Storage URL to write regridded output to.\n    process : {\"pre\", \"post\"}\n        Step in pipeline, used in determining how to correct.\n        \"Pre\" replaces all zero values with a uniform random value below a threshold (before bias adjustment).\n        \"Post\" replaces all values below a threshold with zeroes (after bias adjustment).\n    variable: str\n    \"\"\"\n    ds = storage.read(x)\n    ds_corrected = apply_wet_day_frequency_correction(\n        ds, process=process, variable=variable\n    )\n    storage.write(out, ds_corrected)\n</code></pre>"},{"location":"services/#dodola.services.get_attrs","title":"dodola.services.get_attrs","text":"<pre><code>get_attrs(x, variable=None)\n</code></pre> <p>Get JSON str of <code>x</code> attrs metadata.</p> Source code in <code>dodola/services.py</code> <pre><code>def get_attrs(x, variable=None):\n    \"\"\"Get JSON str of `x` attrs metadata.\"\"\"\n    d = storage.read(x)\n    if variable:\n        d = d[variable]\n    return json.dumps(d.attrs)\n</code></pre>"},{"location":"services/#dodola.services.log_service","title":"dodola.services.log_service","text":"<pre><code>log_service(func)\n</code></pre> <p>Decorator for dodola.services to log service start and stop</p> Source code in <code>dodola/services.py</code> <pre><code>def log_service(func):\n    \"\"\"Decorator for dodola.services to log service start and stop\"\"\"\n\n    @wraps(func)\n    def service_logger(*args, **kwargs):\n        servicename = func.__name__\n        logger.info(f\"Starting dodola service {servicename} with {args=}, {kwargs=})\")\n        func(*args, **kwargs)\n        logger.info(f\"dodola service {servicename} done\")\n\n    return service_logger\n</code></pre>"},{"location":"services/#dodola.services.prime_qdm_output_zarrstore","title":"dodola.services.prime_qdm_output_zarrstore","text":"<pre><code>prime_qdm_output_zarrstore(simulation, variable, years, out, zarr_region_dims, root_attrs_json_file=None, new_attrs=None)\n</code></pre> <p>Init a Zarr Store for writing QDM output regionally in independent processes.</p> <p>Parameters:</p> Name Type Description Default <code>simulation</code> <code>str</code> <p>fsspec-compatible URL containing simulation data to be adjusted.</p> required <code>variable</code> <code>str</code> <p>Target variable in <code>simulation</code> to adjust. Adjusted output will share the same name.</p> required <code>years</code> <code>sequence of ints</code> <p>Years of simulation to adjust, with rolling years and day grouping.</p> required <code>out</code> <code>str</code> <p>fsspec-compatible path or URL pointing to Zarr Store file where the QDM-adjusted simulation data will be written.</p> required <code>zarr_region_dims</code> <p>Sequence giving the name of dimensions that will be used to later write to regions of the Zarr Store. Variables with dimensions that do not use these regional variables will be appended to the primed Zarr Store as part of this call.</p> required <code>root_attrs_json_file</code> <code>str or None</code> <p>fsspec-compatible URL pointing to a JSON file to use as root <code>attrs</code> for the output data. <code>new_attrs</code> will be appended to this.</p> <code>None</code> <code>new_attrs</code> <code>dict or None</code> <p>dict to merge with output Dataset's root <code>attrs</code> before output.</p> <code>None</code> Source code in <code>dodola/services.py</code> <pre><code>@log_service\ndef prime_qdm_output_zarrstore(\n    simulation,\n    variable,\n    years,\n    out,\n    zarr_region_dims,\n    root_attrs_json_file=None,\n    new_attrs=None,\n):\n    \"\"\"Init a Zarr Store for writing QDM output regionally in independent processes.\n\n    Parameters\n    ----------\n    simulation : str\n        fsspec-compatible URL containing simulation data to be adjusted.\n    variable : str\n        Target variable in `simulation` to adjust. Adjusted output will share the\n        same name.\n    years : sequence of ints\n        Years of simulation to adjust, with rolling years and day grouping.\n    out : str\n        fsspec-compatible path or URL pointing to Zarr Store file where the\n        QDM-adjusted simulation data will be written.\n    zarr_region_dims: sequence of str\n        Sequence giving the name of dimensions that will be used to later write\n        to regions of the Zarr Store. Variables with dimensions that do not use\n        these regional variables will be appended to the primed Zarr Store as\n        part of this call.\n    root_attrs_json_file : str or None, optional\n        fsspec-compatible URL pointing to a JSON file to use as root ``attrs``\n        for the output data. ``new_attrs`` will be appended to this.\n    new_attrs : dict or None, optional\n        dict to merge with output Dataset's root ``attrs`` before output.\n    \"\"\"\n    # TODO: Options to change primed output zarr store chunking?\n    import xarray as xr  # TODO: Clean up this import or move the import-depending code to doodla.core\n\n    quantile_variable_name = \"sim_q\"\n    sim_df = storage.read(simulation)\n\n    if root_attrs_json_file:\n        logger.info(f\"Using root attrs from {root_attrs_json_file}\")\n        sim_df.attrs = storage.read_attrs(root_attrs_json_file)\n\n    # Yes, the time slice needs to use strs, not ints. It's already going to be inclusive so don't need to +1.\n    primer = sim_df.sel(time=slice(str(min(years)), str(max(years))))\n\n    ## This is where chunking happens... not sure about whether this is needed or how to effectively handle this.\n    # primed_out = dodola.repository.read(simulation_zarr).sel(time=timeslice).chunk({\"time\": 73, \"lat\": 10, \"lon\":180})\n\n    primer[quantile_variable_name] = xr.zeros_like(primer[variable])\n    # Analysts said sim_q needed no attrs.\n    primer[quantile_variable_name].attrs = {}\n\n    if new_attrs:\n        primer.attrs |= new_attrs\n\n    # Logic below might be better off in dodola.repository.\n    logger.debug(f\"Priming Zarr Store with {primer=}\")\n    primer.to_zarr(\n        out,\n        mode=\"w\",\n        compute=False,\n        consolidated=True,\n        safe_chunks=False,\n    )\n    logger.info(f\"Written primer to {out}\")\n\n    # Append variables that do not depend on dims we're using to define the\n    # region we'll later write to in the Zarr Store.\n    variables_to_append = []\n    for variable_name, variable in primer.variables.items():\n        if any(\n            region_variable not in variable.dims for region_variable in zarr_region_dims\n        ):\n            variables_to_append.append(variable_name)\n\n    if variables_to_append:\n        logger.info(f\"Appending {variables_to_append} to primed Zarr Store\")\n        primer[variables_to_append].to_zarr(\n            out, mode=\"a\", compute=True, consolidated=True, safe_chunks=False\n        )\n        logger.info(f\"Appended non-regional variables to {out}\")\n    else:\n        logger.info(\"No non-regional variables to append to Zarr Store\")\n</code></pre>"},{"location":"services/#dodola.services.prime_qplad_output_zarrstore","title":"dodola.services.prime_qplad_output_zarrstore","text":"<pre><code>prime_qplad_output_zarrstore(simulation, variable, out, zarr_region_dims, root_attrs_json_file=None, new_attrs=None)\n</code></pre> <p>Init a Zarr Store for writing QPLAD output regionally in independent processes.</p> <p>Parameters:</p> Name Type Description Default <code>simulation</code> <code>str</code> <p>fsspec-compatible URL containing simulation data to be adjusted.</p> required <code>variable</code> <code>str</code> <p>Target variable in <code>simulation</code> to adjust. Adjusted output will share the same name.</p> required <code>out</code> <code>str</code> <p>fsspec-compatible path or URL pointing to Zarr Store file where the QPLAD-adjusted simulation data will be written.</p> required <code>zarr_region_dims</code> <p>Sequence giving the name of dimensions that will be used to later write to regions of the Zarr Store. Variables with dimensions that do not use these regional variables will be appended to the primed Zarr Store as part of this call.</p> required <code>root_attrs_json_file</code> <code>str or None</code> <p>fsspec-compatible URL pointing to a JSON file to use as root <code>attrs</code> for the output data. <code>new_attrs</code> will be appended to this.</p> <code>None</code> <code>new_attrs</code> <code>dict or None</code> <p>dict to merge with output Dataset's root <code>attrs</code> before output.</p> <code>None</code> Source code in <code>dodola/services.py</code> <pre><code>@log_service\ndef prime_qplad_output_zarrstore(\n    simulation,\n    variable,\n    out,\n    zarr_region_dims,\n    root_attrs_json_file=None,\n    new_attrs=None,\n):\n    \"\"\"Init a Zarr Store for writing QPLAD output regionally in independent processes.\n\n    Parameters\n    ----------\n    simulation : str\n        fsspec-compatible URL containing simulation data to be adjusted.\n    variable : str\n        Target variable in `simulation` to adjust. Adjusted output will share the\n        same name.\n    out : str\n        fsspec-compatible path or URL pointing to Zarr Store file where the\n        QPLAD-adjusted simulation data will be written.\n    zarr_region_dims: sequence of str\n        Sequence giving the name of dimensions that will be used to later write\n        to regions of the Zarr Store. Variables with dimensions that do not use\n        these regional variables will be appended to the primed Zarr Store as\n        part of this call.\n    root_attrs_json_file : str or None, optional\n        fsspec-compatible URL pointing to a JSON file to use as root ``attrs``\n        for the output data. ``new_attrs`` will be appended to this.\n    new_attrs : dict or None, optional\n        dict to merge with output Dataset's root ``attrs`` before output.\n    \"\"\"\n    sim_df = storage.read(simulation)\n\n    if root_attrs_json_file:\n        logger.info(f\"Using root attrs from {root_attrs_json_file}\")\n        sim_df.attrs = storage.read_attrs(root_attrs_json_file)\n\n    primer = sim_df[[variable]]\n    # Ensure we get root attrs. Not sure explicit copy is still required.\n    primer.attrs = sim_df.attrs.copy()\n\n    if new_attrs:\n        primer.attrs |= new_attrs\n\n    # Logic below might be better off in dodola.repository.\n    logger.debug(f\"Priming Zarr Store with {primer=}\")\n    primer.to_zarr(out, mode=\"w\", compute=False, consolidated=True)\n    logger.info(f\"Written primer to {out}\")\n\n    # Append variables that do not depend on dims we're using to define the\n    # region we'll later write to in the Zarr Store.\n    variables_to_append = []\n    for variable_name, variable in primer.variables.items():\n        if any(\n            region_variable not in variable.dims for region_variable in zarr_region_dims\n        ):\n            variables_to_append.append(variable_name)\n\n    if variables_to_append:\n        logger.info(f\"Appending {variables_to_append} to primed Zarr Store\")\n        primer[variables_to_append].to_zarr(\n            out, mode=\"a\", compute=True, consolidated=True, safe_chunks=False\n        )\n        logger.info(f\"Appended non-regional variables to {out}\")\n    else:\n        logger.info(\"No non-regional variables to append to Zarr Store\")\n</code></pre>"},{"location":"services/#dodola.services.rechunk","title":"dodola.services.rechunk","text":"<pre><code>rechunk(x, target_chunks, out)\n</code></pre> <p>Rechunk data to specification</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>str</code> <p>Storage URL to input data.</p> required <code>target_chunks</code> <code>dict</code> <p>Mapping {coordinate_name: chunk_size} showing how data is to be rechunked.</p> required <code>out</code> <code>str</code> <p>Storage URL to write rechunked output to.</p> required Source code in <code>dodola/services.py</code> <pre><code>@log_service\ndef rechunk(x, target_chunks, out):\n    \"\"\"Rechunk data to specification\n\n    Parameters\n    ----------\n    x : str\n        Storage URL to input data.\n    target_chunks : dict\n        Mapping {coordinate_name: chunk_size} showing how data is\n        to be rechunked.\n    out : str\n        Storage URL to write rechunked output to.\n    \"\"\"\n    ds = storage.read(x)\n\n    # Simple, stable, but not for more specialized rechunking needs.\n    # In that case use \"rechunker\" package, or similar.\n    ds = ds.chunk(target_chunks)\n\n    # Hack to get around issue with writing chunks to zarr in xarray ~v0.17.0\n    # https://github.com/pydata/xarray/issues/2300\n    for v in ds.data_vars.keys():\n        del ds[v].encoding[\"chunks\"]\n\n    storage.write(out, ds)\n</code></pre>"},{"location":"services/#dodola.services.regrid","title":"dodola.services.regrid","text":"<pre><code>regrid(x, out, method, domain_file, weights_path=None, astype=None, add_cyclic=None)\n</code></pre> <p>Regrid climate data</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>str</code> <p>Storage URL to input xr.Dataset that will be regridded.</p> required <code>out</code> <code>str</code> <p>Storage URL to write regridded output to.</p> required <code>method</code> <code>str</code> <p>Method of regridding. Passed to <code>xesmf.Regridder</code>.</p> required <code>domain_file</code> <code>str</code> <p>Storage URL to input xr.Dataset domain file to regrid to.</p> required <code>weights_path</code> <code>optional</code> <p>Local file path name to write regridding weights file to.</p> <code>None</code> <code>astype</code> <code>str, numpy.dtype, or None</code> <p>Typecode or data-type to which the regridded output is cast.</p> <code>None</code> <code>add_cyclic</code> <code>str, or None</code> <p>Add cyclic (aka wrap-around values) to dimension before regridding.  Useful for avoiding dateline artifacts along longitude in global  datasets.</p> <code>None</code> Source code in <code>dodola/services.py</code> <pre><code>@log_service\ndef regrid(\n    x, out, method, domain_file, weights_path=None, astype=None, add_cyclic=None\n):\n    \"\"\"Regrid climate data\n\n    Parameters\n    ----------\n    x : str\n        Storage URL to input xr.Dataset that will be regridded.\n    out : str\n        Storage URL to write regridded output to.\n    method : str\n        Method of regridding. Passed to ``xesmf.Regridder``.\n    domain_file : str\n        Storage URL to input xr.Dataset domain file to regrid to.\n    weights_path : optional\n        Local file path name to write regridding weights file to.\n    astype : str, numpy.dtype, or None, optional\n        Typecode or data-type to which the regridded output is cast.\n    add_cyclic : str, or None, optional\n        Add cyclic (aka wrap-around values) to dimension before regridding.\n         Useful for avoiding dateline artifacts along longitude in global\n         datasets.\n    \"\"\"\n    ds = storage.read(x)\n    ds_domain = storage.read(domain_file)\n\n    regridded_ds = xesmf_regrid(\n        ds,\n        ds_domain,\n        method=method,\n        weights_path=weights_path,\n        astype=astype,\n        add_cyclic=add_cyclic,\n    )\n\n    storage.write(out, regridded_ds)\n</code></pre>"},{"location":"services/#dodola.services.remove_leapdays","title":"dodola.services.remove_leapdays","text":"<pre><code>remove_leapdays(x, out)\n</code></pre> <p>Removes leap days and updates calendar attribute</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>str</code> <p>Storage URL to input xr.Dataset that will be regridded.</p> required <code>out</code> <code>str</code> <p>Storage URL to write regridded output to.</p> required Source code in <code>dodola/services.py</code> <pre><code>@log_service\ndef remove_leapdays(x, out):\n    \"\"\"Removes leap days and updates calendar attribute\n\n    Parameters\n    ----------\n    x : str\n        Storage URL to input xr.Dataset that will be regridded.\n    out : str\n        Storage URL to write regridded output to.\n    \"\"\"\n    ds = storage.read(x)\n    noleap_ds = xclim_remove_leapdays(ds)\n    storage.write(out, noleap_ds)\n</code></pre>"},{"location":"services/#dodola.services.train_qdm","title":"dodola.services.train_qdm","text":"<pre><code>train_qdm(historical, reference, out, variable, kind, sel_slice=None, isel_slice=None)\n</code></pre> <p>Train quantile delta mapping and dump to <code>out</code></p> <p>Parameters:</p> Name Type Description Default <code>historical</code> <code>str</code> <p>fsspec-compatible URL to historical simulation store.</p> required <code>reference</code> <code>str</code> <p>fsspec-compatible URL to store to use as model reference.</p> required <code>out</code> <code>str</code> <p>fsspec-compatible URL to store trained model.</p> required <code>variable</code> <code>str</code> <p>Name of target variable in input and output stores.</p> required <code>kind</code> <code>(additive, multiplicative)</code> <p>Kind of QDM scaling.</p> <code>\"additive\"</code> <code>sel_slice</code> <p>Label-index slice hist and ref to subset before training. A mapping of {variable_name: slice(...)} passed to <code>xarray.Dataset.sel()</code>.</p> <code>None</code> <code>isel_slice</code> <p>Integer-index slice hist and ref to subset before training. A mapping of {variable_name: slice(...)} passed to <code>xarray.Dataset.isel()</code>.</p> <code>None</code> Source code in <code>dodola/services.py</code> <pre><code>@log_service\ndef train_qdm(\n    historical, reference, out, variable, kind, sel_slice=None, isel_slice=None\n):\n    \"\"\"Train quantile delta mapping and dump to `out`\n\n    Parameters\n    ----------\n    historical : str\n        fsspec-compatible URL to historical simulation store.\n    reference : str\n        fsspec-compatible URL to store to use as model reference.\n    out : str\n        fsspec-compatible URL to store trained model.\n    variable : str\n        Name of target variable in input and output stores.\n    kind : {\"additive\", \"multiplicative\"}\n        Kind of QDM scaling.\n    sel_slice: dict or None, optional\n        Label-index slice hist and ref to subset before training.\n        A mapping of {variable_name: slice(...)} passed to\n        `xarray.Dataset.sel()`.\n    isel_slice: dict or None, optional\n        Integer-index slice hist and ref to subset before training. A mapping\n        of {variable_name: slice(...)} passed to `xarray.Dataset.isel()`.\n    \"\"\"\n    hist = storage.read(historical)\n    ref = storage.read(reference)\n\n    kind_map = {\"additive\": \"+\", \"multiplicative\": \"*\"}\n    try:\n        k = kind_map[kind]\n    except KeyError:\n        # So we get a helpful exception message showing accepted kwargs...\n        raise ValueError(f\"kind must be {set(kind_map.keys())}, got {kind}\")\n\n    if sel_slice:\n        logger.info(f\"Slicing by {sel_slice=}\")\n        hist = hist.sel(sel_slice)\n        ref = ref.sel(sel_slice)\n\n    if isel_slice:\n        logger.info(f\"Slicing by {isel_slice=}\")\n        hist = hist.isel(isel_slice)\n        ref = ref.isel(isel_slice)\n\n    ref = xclim_units_any2pint(ref, variable)\n    hist = xclim_units_any2pint(hist, variable)\n\n    qdm = train_quantiledeltamapping(\n        reference=ref, historical=hist, variable=variable, kind=k\n    )\n\n    storage.write(out, qdm.ds)\n</code></pre>"},{"location":"services/#dodola.services.train_qplad","title":"dodola.services.train_qplad","text":"<pre><code>train_qplad(coarse_reference, fine_reference, out, variable, kind, sel_slice=None, isel_slice=None)\n</code></pre> <p>Train Quantile-Preserving, Localized Analogs Downscaling and dump to <code>out</code></p> <p>Parameters:</p> Name Type Description Default <code>coarse_reference</code> <code>str</code> <p>fsspec-compatible URL to resampled coarse reference store.</p> required <code>fine_reference</code> <code>str</code> <p>fsspec-compatible URL to fine-resolution reference store.</p> required <code>out</code> <code>str</code> <p>fsspec-compatible URL to store adjustment factors.</p> required <code>variable</code> <code>str</code> <p>Name of target variable in input and output stores.</p> required <code>kind</code> <code>(additive, multiplicative)</code> <p>Kind of QPLAD downscaling.</p> <code>\"additive\"</code> <code>sel_slice</code> <p>Label-index slice hist and ref to subset before training. A mapping of {variable_name: slice(...)} passed to <code>xarray.Dataset.sel()</code>.</p> <code>None</code> <code>isel_slice</code> <p>Integer-index slice hist and ref to subset before training. A mapping of {variable_name: slice(...)} passed to <code>xarray.Dataset.isel()</code>.</p> <code>None</code> Source code in <code>dodola/services.py</code> <pre><code>@log_service\ndef train_qplad(\n    coarse_reference,\n    fine_reference,\n    out,\n    variable,\n    kind,\n    sel_slice=None,\n    isel_slice=None,\n):\n    \"\"\"Train Quantile-Preserving, Localized Analogs Downscaling and dump to `out`\n\n    Parameters\n    ----------\n    coarse_reference : str\n        fsspec-compatible URL to resampled coarse reference store.\n    fine_reference : str\n        fsspec-compatible URL to fine-resolution reference store.\n    out : str\n        fsspec-compatible URL to store adjustment factors.\n    variable : str\n        Name of target variable in input and output stores.\n    kind : {\"additive\", \"multiplicative\"}\n        Kind of QPLAD downscaling.\n    sel_slice: dict or None, optional\n        Label-index slice hist and ref to subset before training.\n        A mapping of {variable_name: slice(...)} passed to\n        `xarray.Dataset.sel()`.\n    isel_slice: dict or None, optional\n        Integer-index slice hist and ref to subset before training. A mapping\n        of {variable_name: slice(...)} passed to `xarray.Dataset.isel()`.\n    \"\"\"\n    ref_coarse = storage.read(coarse_reference)\n    ref_fine = storage.read(fine_reference)\n\n    kind_map = {\"additive\": \"+\", \"multiplicative\": \"*\"}\n    try:\n        k = kind_map[kind]\n    except KeyError:\n        # So we get a helpful exception message showing accepted kwargs...\n        raise ValueError(f\"kind must be {set(kind_map.keys())}, got {kind}\")\n\n    if sel_slice:\n        logger.info(f\"Slicing by {sel_slice=}\")\n        ref_coarse = ref_coarse.sel(sel_slice)\n        ref_fine = ref_fine.sel(sel_slice)\n\n    if isel_slice:\n        logger.info(f\"Slicing by {isel_slice=}\")\n        ref_coarse = ref_coarse.isel(isel_slice)\n        ref_fine = ref_fine.isel(isel_slice)\n\n    # needs to not be chunked\n    ref_coarse.load()\n    ref_fine.load()\n\n    ref_coarse = xclim_units_any2pint(ref_coarse, variable)\n    ref_fine = xclim_units_any2pint(ref_fine, variable)\n\n    qplad = train_analogdownscaling(\n        coarse_reference=ref_coarse,\n        fine_reference=ref_fine,\n        variable=variable,\n        kind=k,\n    )\n\n    storage.write(out, qplad.ds)\n</code></pre>"},{"location":"services/#dodola.services.validate","title":"dodola.services.validate","text":"<pre><code>validate(x, var, data_type, time_period)\n</code></pre> <p>Performs validation on an input dataset</p> <p>Valid for CMIP6, bias corrected and downscaled. Raises AssertionError when validation fails.</p> <p>This function performs more memory-intensive tests by reading input data and subsetting to each year in the \"time\" dimension.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>str</code> <p>Storage URL to input xr.Dataset that will be validated.</p> required <code>var</code> <code>(tasmax, tasmin, dtr, pr)</code> <p>Variable in xr.Dataset that should be validated. Some validation functions are specific to each variable.</p> <code>\"tasmax\"</code> <code>data_type</code> <code>(cmip6, bias_corrected, downscaled)</code> <p>Step in pipeline, used in determining how to validate.</p> <code>\"cmip6\"</code> <code>time_period</code> <p>Time period that input data should cover, used in validating the number of timesteps in conjunction with the data type.</p> required Source code in <code>dodola/services.py</code> <pre><code>@log_service\ndef validate(x, var, data_type, time_period):\n    \"\"\"Performs validation on an input dataset\n\n    Valid for CMIP6, bias corrected and downscaled. Raises AssertionError when\n    validation fails.\n\n    This function performs more memory-intensive tests by reading input data\n    and subsetting to each year in the \"time\" dimension.\n\n    Parameters\n    ----------\n    x : str\n        Storage URL to input xr.Dataset that will be validated.\n    var : {\"tasmax\", \"tasmin\", \"dtr\", \"pr\"}\n        Variable in xr.Dataset that should be validated.\n        Some validation functions are specific to each variable.\n    data_type : {\"cmip6\", \"bias_corrected\", \"downscaled\"}\n        Step in pipeline, used in determining how to validate.\n    time_period: {\"historical\", \"future\"}\n        Time period that input data should cover, used in validating the number of timesteps\n        in conjunction with the data type.\n    \"\"\"\n    # This is pretty rough but works to communicate the idea.\n    # Consider having failed tests raise something like ValidationError rather\n    # than AssertionErrors.\n    ds = storage.read(x)\n\n    # These only read in Zarr Store metadata -- not memory intensive.\n    test_variable_names(ds, var)\n    test_timesteps(ds, data_type, time_period)\n\n    # Other test are done on annual selections with dask.delayed to\n    # avoid large memory errors.\n    # Doing all this here because this involves storage and I/O logic.\n    @dask.delayed\n    def memory_intensive_tests(f, v, t):\n        d = storage.read(f).sel(time=str(t))\n\n        test_for_nans(d, v)\n\n        if v == \"tasmin\":\n            test_temp_range(d, v)\n        elif v == \"tasmax\":\n            test_temp_range(d, v)\n        elif v == \"dtr\":\n            test_dtr_range(d, v, data_type)\n            test_negative_values(d, v)\n        elif v == \"pr\":\n            test_negative_values(d, v)\n            test_maximum_precip(d, v)\n        else:\n            raise ValueError(f\"Argument {v=} not recognized\")\n\n        # Assumes error thrown if had problem before this.\n        return True\n\n    tasks = []\n    for t in set(ds[\"time\"].dt.year.data):\n        logger.debug(f\"Validating year {t}\")\n        tasks.append(memory_intensive_tests(x, var, t))\n    tasks = dask.compute(*tasks)\n    assert all(tasks)  # Likely don't need this\n</code></pre>"}]}